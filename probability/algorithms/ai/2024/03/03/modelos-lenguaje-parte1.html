<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Entendiendo los Modelos del Lenguaje (Parte 1) | Mr Dipalma‚Äôs Pub üç∫</title>
<meta name="generator" content="Jekyll v3.10.0">
<meta property="og:title" content="Entendiendo los Modelos del Lenguaje (Parte 1)">
<meta name="author" content="dpalmasan">
<meta property="og:locale" content="en_US">
<meta name="description" content="Este es un blog donde compartir√© un poco sobre m√≠ y mis experiencias en el mundo tecnol√≥gico‚Ä¶">
<meta property="og:description" content="Este es un blog donde compartir√© un poco sobre m√≠ y mis experiencias en el mundo tecnol√≥gico‚Ä¶">
<link rel="canonical" href="https://dpalmasan.github.io/website/probability/algorithms/ai/2024/03/03/modelos-lenguaje-parte1.html">
<meta property="og:url" content="https://dpalmasan.github.io/website/probability/algorithms/ai/2024/03/03/modelos-lenguaje-parte1.html">
<meta property="og:site_name" content="Mr Dipalma‚Äôs Pub üç∫">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2024-03-03T19:30:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Entendiendo los Modelos del Lenguaje (Parte 1)">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"dpalmasan"},"dateModified":"2024-03-03T19:30:00+00:00","datePublished":"2024-03-03T19:30:00+00:00","description":"Este es un blog donde compartir√© un poco sobre m√≠ y mis experiencias en el mundo tecnol√≥gico‚Ä¶","headline":"Entendiendo los Modelos del Lenguaje (Parte 1)","mainEntityOfPage":{"@type":"WebPage","@id":"https://dpalmasan.github.io/website/probability/algorithms/ai/2024/03/03/modelos-lenguaje-parte1.html"},"url":"https://dpalmasan.github.io/website/probability/algorithms/ai/2024/03/03/modelos-lenguaje-parte1.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css">
    <link rel="stylesheet" href="/website/assets/css/style.css">
    <style type="text/css">
        div#disqus_thread iframe[sandbox] {
                max-height: 0px !important;
        }
    </style>
<link type="application/atom+xml" rel="alternate" href="https://dpalmasan.github.io/website/feed.xml" title="Mr Dipalma's Pub üç∫">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-KHXBX5G1VP"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){window.dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-KHXBX5G1VP');
</script>

<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<header class="site-header">

  <div class="wrapper">
<a class="site-title" rel="author" href="/website/">Mr Dipalma's Pub üç∫</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewbox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/website/about/">About</a></div>
      </nav>
</div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Entendiendo los Modelos del Lenguaje (Parte 1)</h1>
    <p class="post-meta"><time class="dt-published" datetime="2024-03-03T19:30:00+00:00" itemprop="datePublished">
        Mar 3, 2024
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <div align="center">
<p><img src="https://gist.githubusercontent.com/dpalmasan/103d61ae06cfd3e7dee7888b391c1792/raw/78da1f25307f7487f5e6287bcd6640096de80b58/ai.png" alt="ai"></p>
</div>
<h1 id="introduccin">Introducci√≥n</h1>
<p>Iniciar√© una serie de art√≠culos, corta igual que la √∫ltima que hice sobre <em>De vuelta a lo B√°sico</em>, donde expliqu√© probabilidades y conceptos b√°sicos de modelos probabil√≠sticos. Si quieres revisar dichos art√≠culos, dejo los siguientes enlaces:</p>
<ul>
<li><a href="/website/probability/algorithms/ai/2024/02/25/de-vuelta-a-lo-basico.html"><em>De vuelta a lo b√°sico (Parte 1)</em></a></li>
<li><a href="/website/probability/algorithms/ai/2024/02/26/variables-aleatorias.html"><em>De vuelta a lo b√°sico (Parte 2)</em></a></li>
<li><a href="/website/probability/algorithms/ai/2024/02/28/hipotesis-correlacion.html"><em>De vuelta a lo b√°sico (Parte 3)</em></a></li>
</ul>
<p>Esta nueva serie de art√≠culos intentar√© <em>aclarar</em> y desmenuzar los modelos de lenguaje, entendiendo primero qu√© es un modelo de lenguaje, y algunos temas de ling√º√≠stica que pueden ser interesantes. Todo esto, con el fin de transferir conocimiento y evitar <em>mal entendimiento</em> de algunos conceptos.</p>
<h1 id="modelos-de-lenguaje">Modelos de Lenguaje</h1>
<h2 id="intuicin">Intuici√≥n</h2>
<p>En t√©rminos simb√≥licos, definimos un vocabulario como un conjunto de s√≠mbolos, que t√≠picamente representan letras, d√≠gitos, fonemas e incluso palabras. Entonces, este conjunto que al menos tiene un elemento se definir√≠a como:</p>
<p>$$\Sigma = \left\{w_1, w_2, \ldots w_N\right\}$$</p>
<p>En el caso de un lenguaje hablado, podr√≠a pensarse como que el i-√©simo elemento $w_i$ es una palabra del lenguaje. Para entender mejor esta idea, consideremos el siguiente vocabulario:</p>
<p>$$\Sigma = \left\{\text{‚Äù.‚Äù}, \text{‚Äúperro‚Äù}, \text{‚Äúcome‚Äù}, \text{‚ÄúEl‚Äù}\right\}$$</p>
<p>Por otro lado, supongamos que vivimos en un mundo donde las oraciones tienen √∫nicamente $4$ palabras. En este caso, podr√≠amos representar las oraciones como una secuencia $\left(w_1, w_2, w_3, w_4\right)$. Para este mundo ficticion, tenemos que existen $4^4 = 256$ posibles combinaciones:</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">product</span>

<span class="n">vocab</span> <span class="o">=</span> <span class="p">(</span><span class="s">"El"</span><span class="p">,</span> <span class="s">"perro"</span><span class="p">,</span> <span class="s">"come"</span><span class="p">,</span> <span class="s">"."</span><span class="p">)</span>
<span class="n">sample_space</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">product</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">repeat</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)))</span>

<span class="k">for</span> <span class="n">perm</span> <span class="ow">in</span> <span class="n">sample_space</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">perm</span><span class="p">)</span>
</code></pre></div></div>
<details><summary>Ver posibles combinaciones</summary>
<pre><code>('El', 'El', 'El', 'El')
('El', 'El', 'El', 'perro')
('El', 'El', 'El', 'come')
('El', 'El', 'El', '.')
('El', 'El', 'perro', 'El')
('El', 'El', 'perro', 'perro')
('El', 'El', 'perro', 'come')
('El', 'El', 'perro', '.')
('El', 'El', 'come', 'El')
('El', 'El', 'come', 'perro')
('El', 'El', 'come', 'come')
('El', 'El', 'come', '.')
('El', 'El', '.', 'El')
('El', 'El', '.', 'perro')
('El', 'El', '.', 'come')
('El', 'El', '.', '.')
('El', 'perro', 'El', 'El')
('El', 'perro', 'El', 'perro')
('El', 'perro', 'El', 'come')
('El', 'perro', 'El', '.')
('El', 'perro', 'perro', 'El')
('El', 'perro', 'perro', 'perro')
('El', 'perro', 'perro', 'come')
('El', 'perro', 'perro', '.')
('El', 'perro', 'come', 'El')
('El', 'perro', 'come', 'perro')
('El', 'perro', 'come', 'come')
('El', 'perro', 'come', '.')
('El', 'perro', '.', 'El')
('El', 'perro', '.', 'perro')
('El', 'perro', '.', 'come')
('El', 'perro', '.', '.')
('El', 'come', 'El', 'El')
('El', 'come', 'El', 'perro')
('El', 'come', 'El', 'come')
('El', 'come', 'El', '.')
('El', 'come', 'perro', 'El')
('El', 'come', 'perro', 'perro')
('El', 'come', 'perro', 'come')
('El', 'come', 'perro', '.')
('El', 'come', 'come', 'El')
('El', 'come', 'come', 'perro')
('El', 'come', 'come', 'come')
('El', 'come', 'come', '.')
('El', 'come', '.', 'El')
('El', 'come', '.', 'perro')
('El', 'come', '.', 'come')
('El', 'come', '.', '.')
('El', '.', 'El', 'El')
('El', '.', 'El', 'perro')
('El', '.', 'El', 'come')
('El', '.', 'El', '.')
('El', '.', 'perro', 'El')
('El', '.', 'perro', 'perro')
('El', '.', 'perro', 'come')
('El', '.', 'perro', '.')
('El', '.', 'come', 'El')
('El', '.', 'come', 'perro')
('El', '.', 'come', 'come')
('El', '.', 'come', '.')
('El', '.', '.', 'El')
('El', '.', '.', 'perro')
('El', '.', '.', 'come')
('El', '.', '.', '.')
('perro', 'El', 'El', 'El')
('perro', 'El', 'El', 'perro')
('perro', 'El', 'El', 'come')
('perro', 'El', 'El', '.')
('perro', 'El', 'perro', 'El')
('perro', 'El', 'perro', 'perro')
('perro', 'El', 'perro', 'come')
('perro', 'El', 'perro', '.')
('perro', 'El', 'come', 'El')
('perro', 'El', 'come', 'perro')
('perro', 'El', 'come', 'come')
('perro', 'El', 'come', '.')
('perro', 'El', '.', 'El')
('perro', 'El', '.', 'perro')
('perro', 'El', '.', 'come')
('perro', 'El', '.', '.')
('perro', 'perro', 'El', 'El')
('perro', 'perro', 'El', 'perro')
('perro', 'perro', 'El', 'come')
('perro', 'perro', 'El', '.')
('perro', 'perro', 'perro', 'El')
('perro', 'perro', 'perro', 'perro')
('perro', 'perro', 'perro', 'come')
('perro', 'perro', 'perro', '.')
('perro', 'perro', 'come', 'El')
('perro', 'perro', 'come', 'perro')
('perro', 'perro', 'come', 'come')
('perro', 'perro', 'come', '.')
('perro', 'perro', '.', 'El')
('perro', 'perro', '.', 'perro')
('perro', 'perro', '.', 'come')
('perro', 'perro', '.', '.')
('perro', 'come', 'El', 'El')
('perro', 'come', 'El', 'perro')
('perro', 'come', 'El', 'come')
('perro', 'come', 'El', '.')
('perro', 'come', 'perro', 'El')
('perro', 'come', 'perro', 'perro')
('perro', 'come', 'perro', 'come')
('perro', 'come', 'perro', '.')
('perro', 'come', 'come', 'El')
('perro', 'come', 'come', 'perro')
('perro', 'come', 'come', 'come')
('perro', 'come', 'come', '.')
('perro', 'come', '.', 'El')
('perro', 'come', '.', 'perro')
('perro', 'come', '.', 'come')
('perro', 'come', '.', '.')
('perro', '.', 'El', 'El')
('perro', '.', 'El', 'perro')
('perro', '.', 'El', 'come')
('perro', '.', 'El', '.')
('perro', '.', 'perro', 'El')
('perro', '.', 'perro', 'perro')
('perro', '.', 'perro', 'come')
('perro', '.', 'perro', '.')
('perro', '.', 'come', 'El')
('perro', '.', 'come', 'perro')
('perro', '.', 'come', 'come')
('perro', '.', 'come', '.')
('perro', '.', '.', 'El')
('perro', '.', '.', 'perro')
('perro', '.', '.', 'come')
('perro', '.', '.', '.')
('come', 'El', 'El', 'El')
('come', 'El', 'El', 'perro')
('come', 'El', 'El', 'come')
('come', 'El', 'El', '.')
('come', 'El', 'perro', 'El')
('come', 'El', 'perro', 'perro')
('come', 'El', 'perro', 'come')
('come', 'El', 'perro', '.')
('come', 'El', 'come', 'El')
('come', 'El', 'come', 'perro')
('come', 'El', 'come', 'come')
('come', 'El', 'come', '.')
('come', 'El', '.', 'El')
('come', 'El', '.', 'perro')
('come', 'El', '.', 'come')
('come', 'El', '.', '.')
('come', 'perro', 'El', 'El')
('come', 'perro', 'El', 'perro')
('come', 'perro', 'El', 'come')
('come', 'perro', 'El', '.')
('come', 'perro', 'perro', 'El')
('come', 'perro', 'perro', 'perro')
('come', 'perro', 'perro', 'come')
('come', 'perro', 'perro', '.')
('come', 'perro', 'come', 'El')
('come', 'perro', 'come', 'perro')
('come', 'perro', 'come', 'come')
('come', 'perro', 'come', '.')
('come', 'perro', '.', 'El')
('come', 'perro', '.', 'perro')
('come', 'perro', '.', 'come')
('come', 'perro', '.', '.')
('come', 'come', 'El', 'El')
('come', 'come', 'El', 'perro')
('come', 'come', 'El', 'come')
('come', 'come', 'El', '.')
('come', 'come', 'perro', 'El')
('come', 'come', 'perro', 'perro')
('come', 'come', 'perro', 'come')
('come', 'come', 'perro', '.')
('come', 'come', 'come', 'El')
('come', 'come', 'come', 'perro')
('come', 'come', 'come', 'come')
('come', 'come', 'come', '.')
('come', 'come', '.', 'El')
('come', 'come', '.', 'perro')
('come', 'come', '.', 'come')
('come', 'come', '.', '.')
('come', '.', 'El', 'El')
('come', '.', 'El', 'perro')
('come', '.', 'El', 'come')
('come', '.', 'El', '.')
('come', '.', 'perro', 'El')
('come', '.', 'perro', 'perro')
('come', '.', 'perro', 'come')
('come', '.', 'perro', '.')
('come', '.', 'come', 'El')
('come', '.', 'come', 'perro')
('come', '.', 'come', 'come')
('come', '.', 'come', '.')
('come', '.', '.', 'El')
('come', '.', '.', 'perro')
('come', '.', '.', 'come')
('come', '.', '.', '.')
('.', 'El', 'El', 'El')
('.', 'El', 'El', 'perro')
('.', 'El', 'El', 'come')
('.', 'El', 'El', '.')
('.', 'El', 'perro', 'El')
('.', 'El', 'perro', 'perro')
('.', 'El', 'perro', 'come')
('.', 'El', 'perro', '.')
('.', 'El', 'come', 'El')
('.', 'El', 'come', 'perro')
('.', 'El', 'come', 'come')
('.', 'El', 'come', '.')
('.', 'El', '.', 'El')
('.', 'El', '.', 'perro')
('.', 'El', '.', 'come')
('.', 'El', '.', '.')
('.', 'perro', 'El', 'El')
('.', 'perro', 'El', 'perro')
('.', 'perro', 'El', 'come')
('.', 'perro', 'El', '.')
('.', 'perro', 'perro', 'El')
('.', 'perro', 'perro', 'perro')
('.', 'perro', 'perro', 'come')
('.', 'perro', 'perro', '.')
('.', 'perro', 'come', 'El')
('.', 'perro', 'come', 'perro')
('.', 'perro', 'come', 'come')
('.', 'perro', 'come', '.')
('.', 'perro', '.', 'El')
('.', 'perro', '.', 'perro')
('.', 'perro', '.', 'come')
('.', 'perro', '.', '.')
('.', 'come', 'El', 'El')
('.', 'come', 'El', 'perro')
('.', 'come', 'El', 'come')
('.', 'come', 'El', '.')
('.', 'come', 'perro', 'El')
('.', 'come', 'perro', 'perro')
('.', 'come', 'perro', 'come')
('.', 'come', 'perro', '.')
('.', 'come', 'come', 'El')
('.', 'come', 'come', 'perro')
('.', 'come', 'come', 'come')
('.', 'come', 'come', '.')
('.', 'come', '.', 'El')
('.', 'come', '.', 'perro')
('.', 'come', '.', 'come')
('.', 'come', '.', '.')
('.', '.', 'El', 'El')
('.', '.', 'El', 'perro')
('.', '.', 'El', 'come')
('.', '.', 'El', '.')
('.', '.', 'perro', 'El')
('.', '.', 'perro', 'perro')
('.', '.', 'perro', 'come')
('.', '.', 'perro', '.')
('.', '.', 'come', 'El')
('.', '.', 'come', 'perro')
('.', '.', 'come', 'come')
('.', '.', 'come', '.')
('.', '.', '.', 'El')
('.', '.', '.', 'perro')
('.', '.', '.', 'come')
('.', '.', '.', '.')
</code></pre>
</details>
<p>En este modelo del mundo, estas combinaciones definir√≠an el <strong>espacio muestral</strong> de las oraciones a poder formar. Supongamos que queremos calcular la probabilidad de la oraci√≥n $\text{‚ÄúEl perro come.‚Äù}$. Tenemos 4 <strong>variables aleatorias</strong> $w_1, w_2, w_3, w_4$, sea $P_1$ tal que:</p>
<p>$$
\begin{align}
P_1 &amp;= P\left(w_1=\text{‚ÄúEl‚Äù}, w_2=\text{‚Äúperro‚Äù}, w_3=\text{‚Äúcome‚Äù}, w_4=\text{‚Äù.‚Äù}\right) \\
&amp;= \frac{1}{256} \\
&amp; \approx 0.004
\end{align}
$$</p>
<p>Supongamos que muestreamos $P_2$ tal que:</p>
<p>$$
\begin{align}
P_2 &amp;= P\left(w_1=\text{‚Äù.‚Äù}, w_2=\text{‚Äúcome‚Äù}, w_3=\text{‚Äúperro‚Äù}, w_4=\text{‚ÄúEl‚Äù}\right) \\
&amp;= \frac{1}{256} \\
&amp; \approx 0.004
\end{align}
$$</p>
<p>Esto ocurre, porque asumimos que $P(w_1, w_2, w_3, w_4)$ es una distribuci√≥n uniforme. En el mundo real, si consideramos el lenguaje espa√±ol, esperar√≠amos que $P_1 \gt P_2$, adem√°s, idealmente $P_2 = 0$, ya que seg√∫n las reglas gramaticales, no ser√≠a parte del lenguaje. A este modelo estad√≠stico de calcular la probabilidad de una secuencia de palabras se le conoce como <strong>modelo de lenguaje</strong>.</p>
<p>Finalmente, aclarar que el vocabulario $\Sigma$ en un lenguaje natural es mucho m√°s grande que el del ejemplo donde la cardinalidad (cantidad de elementos) $|\Sigma| = 4$. Observamos que la cantidad de combinaciones posibles crece exponencialmente con el largo $N$ de texto a considerar y la cardinalidad $|\Sigma|$, que representa la cantidad de posibles asignaciones para las variables $w_1, w_2, \ldots, w_N$.</p>
<h2 id="prediccin-de-la-siguiente-palabra">¬øPredicci√≥n de la siguiente palabra?</h2>
<p>La mayor√≠a de la gente, cuando se habla de modelos del lenguaje habla sobre <em>predicci√≥n de la siguiente palabra</em>. Aclaremos esto, ya que recuerdo una vez que cierta persona (ingeniero de software), mencionaba los <em>LLM</em> (<em>Large Language Models</em>) y hablaba sobre predicci√≥n de la siguiente palabra. Sin embargo, cuando le preguntaron el por qu√©, o la justificaci√≥n, la persona no pudo responder y divag√≥ por varias ramas sin responder a la pregunta.</p>
<p>Si recordamos la probabilidad condicional, y escribiendo considerando $P(A\cap B) = P(A, B)$ tenemos:</p>
<p>$$P(A|B) = \displaystyle \frac{P(A, B)}{P(B)}$$</p>
<p>Consideremos un lenguaje cuyas oraciones son de largo $N$, entonces, el modelo de lenguaje estar√≠a dado por:</p>
<p>$$P(w_1, w_2, \ldots w_N)$$</p>
<p>Utilizando la definici√≥n de probabilidad condicional:</p>
<p>$$P(w_1, w_2, \ldots w_N) = P(w_N|w_1, w_2, \ldots w_{N-1})P(w_1, w_2, \ldots w_{N-1}) $$</p>
<p>Extendiendo:</p>
<p>$$
\begin{align}
P(w_1, w_2, \ldots w_N) &amp;= P(w_N|w_1, w_2, \ldots w_{N-1})P(w_{N-1}|w_1, w_2, \ldots w_{N-2}) P(w_1, w_2, \ldots w_{N-2}) \\
&amp;= \displaystyle \prod_{i=1}^{N} P(w_i|w_1 \ldots w_{i-1})
\end{align}
$$</p>
<p>En este caso, intentamos calcular la probabilidad conjunta $P(w_1, w_2, \ldots w_N)$ como una cadena de probabilidades condicionales. Se eligieron las probabilidades condicionales, de manera de considerar la variable $w_i$ dadas todas las variables $w_i, \ldots\ w_{i-1}$, de ah√≠ que decimos que intentamos calcular la probabilidad $P(w_i|w_1 \ldots w_{i-1})$, que se interpreta como calcular la probabilidad de la palabra $w_i$ dadas todas las palabras previas.</p>
<h2 id="estimar-pw1-w2-ldots-wn">¬øEstimar $P(w_1, w_2, \ldots w_N)$?</h2>
<p>Sabemos que $P(w_1, w_2, \ldots w_N)$ es desconocida, por lo tanto necesitamos estimarla. Para ello, lo que se hace en la pr√°ctica es construir un repositorio gigante de textos, y encontrar las probabilidades condicionales para poder calcular la probabilidad conjunta que representa al lenguaje. El lector podr√° observar, ¬øQu√© me garantiza que podr√© recolectar un conjunto de oraciones que contenga todas las oraciones posibles?. No hay garant√≠a, y de hecho, podr√≠a darse el caso de que el repositorio no contenga una oraci√≥n v√°lida y que a dicha oraci√≥n se le asigne una probabilidad igual a cero. Para evitar esto, se hacen aproximaciones. Si definimos $w_1, w_2 \ldots w_{i-1}$ como el <em>contexto</em> para la palabra $w_i$, entonces podemos escoger no utilizar el contexto completo y por ejemplo considerar √∫nicamente las $n$ palabras previas. En este caso, estar√≠amos calculando $P(w_i|w_{i-n}, \ldots w_{i - 1})$. Este tipo de secuencias se conocen como <em>n-gramas</em>.</p>
<p>En mi art√≠culo <a href="/website/entrevistas/ti/2023/01/08/nlp-intro.html"><em>Un poco de NLP b√°sico (no un tutorial de pytorch/tensor flow)</em></a>, explico c√≥mo se construye un modelo de n-gramas simple, utilizando una <em>cadena de Markov</em>. Sin embargo, en dicho art√≠culo describo una posible soluci√≥n al problema. Actualmente, con los avances en semi-conductores y en las ciencias de la computaci√≥n, ahora existen modelos que son capaces de considerar m√°s contexto, e incluso ponderarlo, por ejemplo considerar ciertas palabras m√°s relevantes que otras para el caso de predecir la siguiente palabra. Por ejemplo si consideramos la oraci√≥n: ‚Äú<em>El gato se sent√≥ en</em>‚Äù, podr√≠amos decir que es probable que sea ‚Äú<em>el sof√°</em>‚Äù. Sin embargo, si tuviesemos el contexto completo: ‚Äú<em>Estabamos haciendo una parrillada en la terraza y vimos que el gato se sent√≥ en</em>‚Äù, en este caso es m√°s probable decir ‚Äú<em>el techo</em>‚Äù que ‚Äú<em>el sof√°</em>‚Äù.</p>
<p>Finalmente, al estimar $P(w_1, w_2, \ldots w_N)$ se obtiene un <em>modelo generativo</em> con el cual se pueden muestrear oraciones/texto que incluso no se vio en el conjunto de entrenamiento.</p>
<p>En siguientes art√≠culos, explicar√© de forma simple c√≥mo funciona uno de los modelos que ha causado mayor sensacionalismo recientemente, hablo de GPT (<em>Generative Pre-Trained Transformers</em>), e idealmente estos art√≠culos podr√°n aclarar algunos temas que lamentablemente ciertos <em>influencers</em> viven publicando y que son enga√±osos.</p>
<h1 id="conclusiones">Conclusiones</h1>
<ul>
<li>Un modelo de lenguaje es simplemente un modelo probabil√≠stico que opera con secuencias.</li>
<li>La probabilidad conjunta de una oraci√≥n puede calcularse como un producto de probabilidades condicionales.</li>
<li>El <em>predecir la siguiente palabra</em> es simplemente una forma de estimar $P(w_1, w_2, \ldots w_N)$.</li>
<li>Este modelo probabil√≠stico es generativo, lo que permite muestrear de $P(w_1, w_2, \ldots w_N)$ incluso oraciones no vistas en el repositorio de datos utilizado para estimar dicha distribuci√≥n de probabilidad.</li>
<li>En la historia han existido distintos enfoques/trucos/t√©cnicas para calcular $P(w_1, w_2, \ldots w_N)$. En los recientes desarrollos GPT es el estado del arte en este problema.</li>
</ul>

  </div>
<div id="disqus_thread"></div>
  <script>
    var disqus_config = function () {
      this.page.url = '';
      this.page.identifier = 'https://dpalmasan.github.io/website/probability/algorithms/ai/2024/03/03/modelos-lenguaje-parte1.html';
    };

    (function() {
      var d = document, s = d.createElement('script');

      s.src = 'https://dpalmasan.disqus.com/embed.js';

      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow noopener noreferrer" target="_blank">comments powered by Disqus.</a>
</noscript>
<a class="u-url" href="/website/probability/algorithms/ai/2024/03/03/modelos-lenguaje-parte1.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/website/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="https://dpalmasan.github.io/website/feed.xml">
            <svg class="svg-icon orange">
              <path d="M12.8 16C12.8 8.978 7.022 3.2 0 3.2V0c8.777 0 16 7.223 16 16h-3.2zM2.194
                11.61c1.21 0 2.195.985 2.195 2.196 0 1.21-.99 2.194-2.2 2.194C.98 16 0 15.017 0
                13.806c0-1.21.983-2.195 2.194-2.195zM10.606
                16h-3.11c0-4.113-3.383-7.497-7.496-7.497v-3.11c5.818 0 10.606 4.79 10.606 10.607z"></path>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">dpalmasan</li>
          
        </ul>
      </div>
      <div class="footer-col">
        <p>Este es un blog donde compartir√© un poco sobre m√≠ y mis experiencias en el mundo tecnol√≥gico...</p>
      </div>
    </div>

    <div class="social-links">
<ul class="social-media-list">
<li>
    <a rel="me noopener noreferrer" href="https://www.linkedin.com/in/dpalmasan/" target="_blank" title="Mi perfil en Linkedin">
      <span class="grey fa-brands fa-linkedin fa-lg"></span>
    </a>
  </li>
<li>
    <a rel="me noopener noreferrer" href="https://www.github.com/dpalmasan" target="_blank" title="Mi Github">
      <span class="grey fa-brands fa-github fa-lg"></span>
    </a>
  </li>
<li>
    <a rel="me noopener noreferrer" href="https://scholar.google.com/citations?user=Y5PN_1AAAAAJ&hl=en" target="_blank" title="Mi Google Scholar">
      <span class="grey fa-brands fa-google-scholar fa-lg"></span>
    </a>
  </li>
<li>
    <a rel="me noopener noreferrer" href="https://stackoverflow.com/users/4051219/dpalma" target="_blank" title="Mis preguntas en SO LOL!">
      <span class="grey fa-brands fa-stack-overflow fa-lg"></span>
    </a>
  </li>
</ul>
</div>

  </div>

</footer>

</body>

</html>
