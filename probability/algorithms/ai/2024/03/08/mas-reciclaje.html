<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Reciclando un poquito m√°s de material viejo | Mr Dipalma‚Äôs Pub üç∫</title>
<meta name="generator" content="Jekyll v3.10.0">
<meta property="og:title" content="Reciclando un poquito m√°s de material viejo">
<meta name="author" content="dpalmasan">
<meta property="og:locale" content="en_US">
<meta name="description" content="Introducci√≥n">
<meta property="og:description" content="Introducci√≥n">
<link rel="canonical" href="https://dpalmasan.github.io/website/probability/algorithms/ai/2024/03/08/mas-reciclaje.html">
<meta property="og:url" content="https://dpalmasan.github.io/website/probability/algorithms/ai/2024/03/08/mas-reciclaje.html">
<meta property="og:site_name" content="Mr Dipalma‚Äôs Pub üç∫">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2024-03-08T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Reciclando un poquito m√°s de material viejo">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"dpalmasan"},"dateModified":"2024-03-08T00:00:00+00:00","datePublished":"2024-03-08T00:00:00+00:00","description":"Introducci√≥n","headline":"Reciclando un poquito m√°s de material viejo","mainEntityOfPage":{"@type":"WebPage","@id":"https://dpalmasan.github.io/website/probability/algorithms/ai/2024/03/08/mas-reciclaje.html"},"url":"https://dpalmasan.github.io/website/probability/algorithms/ai/2024/03/08/mas-reciclaje.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css">
    <link rel="stylesheet" href="/website/assets/css/style.css">
    <style type="text/css">
        div#disqus_thread iframe[sandbox] {
                max-height: 0px !important;
        }
    </style>
<link type="application/atom+xml" rel="alternate" href="https://dpalmasan.github.io/website/feed.xml" title="Mr Dipalma's Pub üç∫">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-KHXBX5G1VP"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){window.dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-KHXBX5G1VP');
</script>

<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"}}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
<header class="site-header">

  <div class="wrapper">
<a class="site-title" rel="author" href="/website/">Mr Dipalma's Pub üç∫</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewbox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/website/about/">About</a></div>
      </nav>
</div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Reciclando un poquito m√°s de material viejo</h1>
    <p class="post-meta"><time class="dt-published" datetime="2024-03-08T00:00:00+00:00" itemprop="datePublished">
        Mar 8, 2024
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="introduccin">Introducci√≥n</h1>
<p>No es un post tan interesante, pero sigue la tem√°tica de reciclar material que alguna vez escrib√≠ y que a alguna persona le podr√° servir, aunque sea para pasar el ocio.</p>
<h1 id="reflexiones-iniciales">Reflexiones Iniciales</h1>
<p>Algunas personas me han preguntado consejos, como por ejemplo qu√© cursos tomar, qu√© hacer para mejorar en X/Y/Z. Aprovechar√© de responder, <em>no tengo la menor idea</em>. Supongo que buscando la literatura e intentando estudiar un poco, el espacio de b√∫squeda se puede reducir.</p>
<h1 id="aprendizaje-no-supervisado">Aprendizaje No Supervisado</h1>
<p>El t√©rmino aprendizaje no supervisado hace referencia a m√©todos que extraen informaci√≥n/patrones en los datos, sin necesidad de que estos hayan sido etiquetados por una persona (o datos en el cual la variable de respuesta es conocida). En mis m√°s recientes art√≠culos, he tocado un poco de aprendizaje supervisado: por ejemplo una regresi√≥n. En estos casos, el objetivo es construir un modelo para predecir una respuesta de inter√©s a partir de un conjunto de variables predictoras. En el caso de aprendizaje no supervisado, tambi√©n se construyen modelos de los datos, pero no hay distinci√≥n entre una variable de respuesta y las variables predictoras.</p>
<p>Algunos ejemplos de aprendizaje no supervisado:</p>
<ol>
<li>Agrupamiento o <em>clustering</em>
</li>
<li>Reducci√≥n dimensional</li>
</ol>
<p>En el caso de agrupamiento, se puede utilizar para identificar grupos en los datos. Por ejemplo, en una aplicaci√≥n web (ej. Amazon, Netflix), se puede tener un sistema de recomendaci√≥n basado en productos, o en usuarios similares (que pueden ser agrupados en base a ciertas caracter√≠sticas).</p>
<p>En el caso de reducci√≥n dimensional, el objetivo puede ser reducir las dimensiones de los datos a un conjunto de variables que sea m√°s manejable. Esta reducci√≥n de variables puede ser utilizada como entrada a modelos predictivos de clasificaci√≥n o regresi√≥n, por ejemplo. O por otro lado, queremos encontrar informaci√≥n subyacente (o latente) en los datos, que puede estar aproximada por m√∫ltiples predictores.</p>
<p>Como ejemplo personal, en mi proyecto open-source <code>TRUNAJOD</code> (<a href="https://github.com/dpalmasan/TRUNAJOD2.0/" target="_blank" rel="noopener noreferrer">Github TRUNAJOD</a>), para explicar al usuario variables de la complejidad textual, b√°sicamente se reduce una gran gama de predictores, en 5 predictores globales de complejidad textual, mediante una t√©cnica llamada an√°lisis factorial.</p>
<h2 id="dimensionalidad">Dimensionalidad</h2>
<p>Los modelos predictivos que hemos visto hasta ahora requieren como entrada un conjunto de datos, que consiste en una variable objetivo y una serie de predictores que idealmente se relacionan con esta variable. Esta serie de predictores, que usualmente son las columnas de nuestro conjunto de datos tabular, pueden ser vistos como vectores matem√°ticos, en donde cada dimension es un atributo (columna).</p>
<p>Podr√≠amos decir, que nuestro conjunto de datos representado por una ‚Äútabla‚Äù de $M\times N$, consiste en $M$ observaciones y $N$ atributos. Esta cantodad $N$ de atributos, es lo que se conoce como <strong>dimensionalidad</strong>. Entonces, en esencia, la <strong>dimensionalidad</strong> de nuestros datos, depender√° de la cantidad de atributos que consideremos por registro.</p>
<p>Es intuitivo pensar que al tener mayor cantidad de atributos (es decir, mayor dimensionalidad), en teor√≠a podr√≠amos tener un mejor modelo, ya que estar√≠amos entregando mayor cantidad de informaci√≥n al modelo predictivo. Sin embargo, al aumentar la dimensionalidad, pueden surgir ciertos inconvenientes:</p>
<ul>
<li>Imaginemos que tenemos $N$ atributos binarios, es decir, tenemos observaciones de la forma $(0, 1, 1, \ldots, 1)$. Para al menos lograr ver todas las combinaciones posibles, necesitariamos $2^N$ registros. Esto es intuitivo, si nuestro modelo requiere m√°s atributos, tendr√° m√°s variabilidad y en consecuencia requerir√° mayor cantidad de registros para poder ajustar un modelo robusto.</li>
<li>Los algoritmo para ajustar modelos tienen complejidades asint√≥ticas (c√≥mo var√≠a cierta m√©trica cuando el tama√±o de la entrada crece) que dependen de $M$ y $N$, por lo tanto, se volver√°n impr√°cticos de ajustar en algunos casos. En otros casos, y esto es intuitivo, el tiempo de ejecuci√≥n aumentar√° (m√°s informaci√≥n que procesar).</li>
<li>Si necesito adem√°s tomar alguna decisi√≥n respecto al an√°lisis de datos ¬øQu√© podr√≠a concluir de un modelo con cientos de atributos? Idealmente, debiese haber alguna forma de reducir la cantidad de dimensiones para facilitar la interpretaci√≥n (ya sea eliminando atributos poco relevantes, o combinando atributos que aproximan propiedades latentes o intr√≠nsecas similares.)</li>
<li>Algo no tan intuitivo, algunos algoritmos para ajuste de modelos padecen lo que se conoce como <strong>maldici√≥n de la dimensionalidad</strong>, es decir que a medida que aumenta la dimensionalidad, el rendimiento comienza a deteriorarse (por ejemplo, aumento en la varianza del error esperado).</li>
</ul>
<p>Existen diferentes manifestaciones de la maldici√≥n de la dimensionalidad, por lo que el lector puede investigar sobre ellas en caso de estar interesado en el tema. Supongamos que queremos implementar un algoritmo de clasificaci√≥n que se base en la similitud de registros para determinar la clase a la que pertenece el registro nuevo (por ejemplo, distancia entre vectores). Supongamos que los datos consisten en puntos distribu√≠dos en un hiper-cubo de dimensi√≥n $p$ (ejemplo: En dos dimensiones ser√≠a un cuadrado de lado 1, en 3 dimensiones un cubo de lado 1, y ya desde 4 dimensiones hacia arriba, no podemos visualizarlo jeje). Consideremos ahora una vecindad hiperc√∫bica de puntos al rededor de un registro objetivo (punto a clasificar), que captura una fracci√≥n $r$ de las observaciones.</p>
<div align="center">
<p><img src="https://raw.githubusercontent.com/dpalmasan/homepage/master/public/imgs/curse_dim.png" alt="dim"></p>
<p><em>Fig 1: Ilustraci√≥n de la maldici√≥n de la dimensionalidad.</em></p>
</div>
<p>Si quisieramos calcular el largo de los lados del hipercubo que contiene una fraci√≥n $r$ del volumen del total de datos, entonces el largo ser√≠a $e_p(r) = r^{1/p}$. Consideremos una dimensionalidad de 10 atributos ($p = 10$), entonces $e_{10}(0.01) = 0.63$ y $e_{10}(0.1) = 0.80$, cuando el rango total de cada entrada (valor de cada atributo) es s√≥lo 1 (hipercubo unitario). Esto quiere decir, que para calcular el $1\%$ o el $10\%$ de los datos para conformar un promedio local, debemos cubrir el $63\%$ o el $80\%$ del rango de cada variable de entrada. Por lo tanto, dichas vecindades, que en dimensionalidades peque√±as eran locales, dejan de ser locales en dimensionalidades altas. Reducir $r$ no ayudar√≠a, pues tendr√≠amos menos observaciones que promediar y por lo tanto la varianza de nuestro ajuste aumentar√≠a.</p>
<p>Por otro lado, se puede demostrar c√≥mo las m√©tricas de distancia se ven afectadas dependiendo de la cantidad de muestras y de la dimensionalidad. Sin embargo, para no complicar la matem√°tica, s√≥lo obtendremos la intuici√≥n de forma experimental. En el siguiente experimento, podemos observar qu√© pasar√≠a con las m√©tricas de distancia, a medida que aumenta dimensionalidad:</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">cdist</span>


<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">plt</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="n">use</span><span class="p">(</span><span class="s">"seaborn"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">"figure.figsize"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">"figure.dpi"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">200</span>


<span class="k">def</span> <span class="nf">get_avg_max_min_dist_ratio</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
    <span class="s">"""Retorna proporcion entre maxima y minima distancia euclideana de un dataset.

    :param dataset: Conjunto de datos.
    :type dataset: pd.numpy.array
    :return: Promedio proporci√≥n de maxima/minima distancia de cada punto
    :rtype: float
    """</span>
    <span class="n">euclidean_dist</span> <span class="o">=</span> <span class="n">cdist</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">dataset</span><span class="p">)</span>

    <span class="c1"># Encontrar minimos que no sean 0
</span>    <span class="n">min_dist</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">euclidean_dist</span><span class="p">):</span>
        <span class="n">min_dist</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">amin</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="n">row</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">])</span>
    <span class="n">max_dist</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">amax</span><span class="p">(</span><span class="n">euclidean_dist</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">max_dist</span><span class="o">/</span><span class="n">min_dist</span><span class="p">)</span>

<span class="c1"># Tama√±o de conjunto de datos
</span><span class="n">M</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">dimensionalities</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">70</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">avg_farthest_distances</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dimensionalities</span><span class="p">))</span>

<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">N</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dimensionalities</span><span class="p">):</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>

    <span class="c1"># Calcular distancia promedio maxima de puntos respecto a cualquier otro punto
</span>    <span class="n">avg_farthest_distances</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_avg_max_min_dist_ratio</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">dimensionalities</span><span class="p">,</span> <span class="n">avg_farthest_distances</span><span class="p">,</span> <span class="s">"o"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Dimensionalidad"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s">"$Promedio\left(\frac\right)$"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Media de proporci√≥n entre distancia m√°xima y m√≠nima </span><span class="se">\n</span><span class="s">vs Dimensionalidad"</span><span class="p">)</span>
</code></pre></div></div>
<div align="center">
<p><img src="https://raw.githubusercontent.com/dpalmasan/homepage/master/public/imgs/dim_dist.png" alt="dim-dist"></p>
<p><em>Fig 2: Proporci√≥n de m√°xima distancia-m√≠nima distancia respecto a la cantidad de dimensiones.</em></p>
</div>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Dimensionalidad 5: </span><span class="si">{</span><span class="n">get_avg_max_min_dist_ratio</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="mi">5</span><span class="p">)))</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Dimensionalidad 500: </span><span class="si">{</span><span class="n">get_avg_max_min_dist_ratio</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="mi">500</span><span class="p">)))</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>
<pre><code>Dimensionalidad 5: 9.33633957667241
Dimensionalidad 500: 1.1948297526920728
</code></pre>
<p>Podemos observar por ejemplo, que en promedio, la proporci√≥n entre la m√°xima y m√≠nima distancia entre los puntos es <code>9.34</code> para <code>N = 5</code> y <code>1.19</code> para <code>N = 500</code>. Esto quiere decir que en el primer caso, el la distancia m√°xima puede llegar a ser hasta 10 veces mayor que la distancia m√≠nima para una dimensionalidad peque√±a, pero s√≥lo de un <code>19%</code> para una dimensionalidad de 500. Esto quiere decir, que en esencia, la informaci√≥n que nos entrega la distancia entre los puntos es casi nula, lo que podr√≠a afectar la varianza en las predicciones aunque el sesgo sea bajo; y por lo tanto, afectar el rendimiento del modelo predictivo.</p>
<h2 id="reduccin-de-dimensionalidad">Reducci√≥n de Dimensionalidad</h2>
<p>Como hemos visto hasta ahora, tener alta dimensionalidad puede causar algunos problemas:</p>
<ul>
<li>Dif√≠cil visualizaci√≥n de datos e interpretaci√≥n (ej. tener demasiadas variables predictoras)</li>
<li>Maldici√≥n de la dimensionalidad, la distancia entre puntos de un espacio tiende a ser insignificante en altas dimensiones.</li>
<li>Variabilidad y dispersi√≥n de datos. Por ejemplo cuando se trabaja con textos y consideramos la frecuencia de diferentes t√©rminos como datos, se trabaja con altas dimensionalidades que adem√°s, por la naturaleza del problema, tienden a generarse matrices dispersas. En estos casos la reducci√≥n dimensional permite reducir la variabilidad en los datos.</li>
<li>En algunos casos, es costoso computacionalmente en t√©rminos de rendimiento y uso de memoria, considerar una elevada cantidad de atributos. Por lo que, la reducci√≥n dimensional permite reducir estos problemas.</li>
</ul>
<p>Existen diferentes m√©todos de reducci√≥n dimensional, pero en este curso veremos en particular dos: <strong>An√°lisis de Componentes Principales</strong>, del ingl√©s PCA <em>Principal Component Analysis</em>, y <strong>An√°lisis Factorial</strong> (<em>Factor Analysis</em>). Estos m√©todos tienen algunos supuestos, que en algunos casos los hacen poco √∫tiles. Por ejemplo para visualizaci√≥n de datos, existen tambi√©n otros m√©todos como: Multidimensional Scaling (MDS), T-SNE, Non-Linear PCA, entre otros.</p>
<div align="center">
<p><img src="https://raw.githubusercontent.com/dpalmasan/homepage/master/public/imgs/factor_vs_pca.png" alt="diff-dim"></p>
<p><em>Fig 3: An√°lisis factorial vs PCA.</em></p>
</div>
<p>El enfoque de PCA para reducci√≥n dimensional es b√°sicamente crear una o m√°s variables a partir de un conjunto de variables medidas. Lo que hace es en esencia crear una combinaci√≥n lineal de estas nuevas variables, que idealmente reproducen las variables medidas. El enfoque en el an√°lisis factorial (utilizado com√∫nmente en psicometr√≠a) es modelar una variable latente/subyacente a partir de un conjunto de mediciones. En simples t√©rminos, los factores a obtener est√°n causando las respuestas en las variables medidas (y sus relaciones), es por ello, que en la figura 3 se muestra la flecha en sentido contrario al caso de PCA. El modelo factorial tambi√©n considera un t√©rmino de error, que en esencia toma la variabilidad que no puede ser explicada √∫nicamente por el factor. En las siguientes secciones se revisar√°n en mayor detalle estos m√©todos.</p>
<h2 id="anlisis-factorial">An√°lisis Factorial</h2>
<p>Los datos multi-variados usualmente son vistos como mediciones indirectas de propiedades subyacentes que no pueden ser medidas directamente. Algunos ejemplos:</p>
<ul>
<li>Pruebas educacionales y psicol√≥gicas utilizan cuestionarios, y usan las respuestas a estos cuestionarios para medir variables subyacentes como la inteligencia u otras habilidades mentales de los sujetos.</li>
<li>Los electroencefalogramas se utilizan para medir actividad neuronal en varias partes del cerebro, mediante mediciones de se√±ales electromagn√©ticas registradas por sensores ubicados en distintas posiciones de la cabeza del sujeto.</li>
<li>Los precios del mercado de acciones cambian constantemente a lo largo del tiempo, y reflejan varios factores que no est√°n medidos, tales como confianza en el mercado, influencias externas, y otras fuerzas que pueden ser dif√≠ciles de identificar o medir.</li>
<li>En caso particular del projecto <code>TRUNAJOD</code>, se intenta medir la complejidad del texto a partir de ciertas propiedades extr√≠nsicas de los mismos. ¬øSe puede medir complejidad textual directamente?</li>
</ul>
<p>El an√°lisis factorial es una t√©cnica estad√≠stica cl√°sica cuyo objetivo es identificar esta informaci√≥n latente (subyacente). Los an√°lisis  factoriales est√°n t√≠picamente ligados a distribuciones Gaussianas, lo que reduce su utilidad en algunos casos.</p>
<p>En esencia los factores est√°n asociados con m√∫ltiples variables observadas, que tienen ciertos patrones similares. Cada factor explica una cantidad particular de la varianza en los datos observados. Esto ayuda en la interpretaci√≥n de los datos, reduciendo la cantidad de variables:</p>
<p>$$X_i = \beta_{i0} + \beta_{i1} F_1 + \ldots \beta_{il}F_l + \varepsilon_i$$</p>
<p>Esto lo podemos visualizar como se muestra en la figura 4.</p>
<div align="center">
<p><img src="https://raw.githubusercontent.com/dpalmasan/homepage/master/public/imgs/factores_ejemplo.png" alt="factorial"></p>
<p><em>Fig 4: Ejemplo de factores.</em></p>
</div>
<p>Cabe destacar que existen dos tipos de an√°lisis factoriales: An√°lisis Factorial Exploratorio y An√°lisis Factorial Confirmatorio. El primero, se enfoca en explorar posibles relaciones, mientras que el segundo se enfoca en confirmarlas (teniendo obviamente una hip√≥tesis de la relaci√≥n de las variables).</p>
<p>Por otro lado, al hacer un an√°lisis factorial, se deben tener las siguientes consideraciones:</p>
<ul>
<li>No hay outliers en los datos.</li>
<li>El tama√±o de la muestra es mayor que la cantidad de factores a considerar.</li>
<li>No debe haber multi-colinealidad (una columna sea combinaci√≥n lineal de otra).</li>
<li>No existe homocedasticidad entre las variables.</li>
</ul>
<p>Analizaremos los datos de la encuesta del Centro de Estudios P√∫blicos realizada en Junio del 2003. Parte del conjunto de preguntas est√° asociado a preguntas sobre el nivel de confianza institucional. Para m√°s informaci√≥n pueden revisar este enlace: <a href="https://www.cepchile.cl/cep/encuestas-cep/encuestas-1998-2008/estudio-nacional-de-opinion-publica-junio-julio-2003" target="_blank" rel="noopener noreferrer">Estudio Nacional de Opini√≥n P√∫blica N¬∞45, Junio-Julio 2003</a>.</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">factor_analyzer</span> <span class="k">as</span> <span class="n">factor</span>
<span class="kn">import</span> <span class="nn">missingno</span> <span class="k">as</span> <span class="n">msngo</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>


<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"datos/cep45.csv"</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>
<p>Extraeremos los datos de las preguntas (valores 8 y 9 significa que no hay informaci√≥n, ver detalle en el enlace mencionado):</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trust_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nb">filter</span><span class="p">(</span><span class="n">regex</span><span class="o">=</span><span class="s">"p17_*"</span><span class="p">)</span>
<span class="n">trust_df</span> <span class="o">=</span> <span class="n">trust_df</span><span class="p">.</span><span class="n">rename</span><span class="p">(</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">{</span>
        <span class="s">"p17_a"</span><span class="p">:</span> <span class="s">"I.Catolica"</span><span class="p">,</span>
        <span class="s">"p17_b"</span><span class="p">:</span> <span class="s">"I.Evangelica"</span><span class="p">,</span>
        <span class="s">"p17_c"</span><span class="p">:</span> <span class="s">"FFAA"</span><span class="p">,</span>
        <span class="s">"p17_d"</span><span class="p">:</span> <span class="s">"Justicia"</span><span class="p">,</span>
        <span class="s">"p17_e"</span><span class="p">:</span> <span class="s">"Prensa"</span><span class="p">,</span>
        <span class="s">"p17_f"</span><span class="p">:</span> <span class="s">"Television"</span><span class="p">,</span>
        <span class="s">"p17_g"</span><span class="p">:</span><span class="s">"Sindicatos"</span><span class="p">,</span>
        <span class="s">"p17_h"</span><span class="p">:</span><span class="s">"Carabineros"</span><span class="p">,</span>
        <span class="s">"p17_i"</span><span class="p">:</span> <span class="s">"Gobierno"</span><span class="p">,</span>
        <span class="s">"p17_j"</span><span class="p">:</span> <span class="s">"PartidosPol"</span><span class="p">,</span>
        <span class="s">"p17_k"</span><span class="p">:</span> <span class="s">"Congreso"</span><span class="p">,</span>
        <span class="s">"p17_l"</span><span class="p">:</span><span class="s">"Empresas"</span><span class="p">,</span>
        <span class="s">"p17_m"</span><span class="p">:</span><span class="s">"Universidades"</span><span class="p">,</span>
        <span class="s">"p17_n"</span><span class="p">:</span><span class="s">"Radio"</span>
<span class="p">})</span>
</code></pre></div></div>
<p>Podemos explorar la base de datos para visualizar los registros incompletos:</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">plt</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="n">use</span><span class="p">(</span><span class="s">"seaborn"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">"figure.figsize"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">"figure.dpi"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">200</span>

<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">msngo</span><span class="p">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">trust_df</span><span class="p">.</span><span class="n">replace</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">]))</span>
</code></pre></div></div>
<div align="center">
<p><img src="https://raw.githubusercontent.com/dpalmasan/homepage/master/public/imgs/missing.png" alt="cep"></p>
<p><em>Fig 5: Visualizaci√≥n de datos incompletos.</em></p>
</div>
<p>Ahora analicemos las medias para cada pregunta:</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">trust_df</span><span class="p">.</span><span class="n">replace</span><span class="p">([</span><span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">trust_df</span><span class="p">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">means</span> <span class="o">=</span> <span class="n">trust_df</span><span class="p">.</span><span class="n">mean</span><span class="p">().</span><span class="n">sort_values</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">means</span><span class="p">.</span><span class="n">values</span><span class="p">,</span> <span class="n">means</span><span class="p">.</span><span class="n">index</span><span class="p">,</span> <span class="s">"bo"</span><span class="p">)</span>
</code></pre></div></div>
<div align="center">
<p><img src="https://raw.githubusercontent.com/dpalmasan/homepage/master/public/imgs/means_atributos.png" alt="cep"></p>
<p><em>Fig 6: Media de los atributos del ejemplo.</em></p>
</div>
<p>Mientras mayor sea el valor, significa que los encuestados conf√≠an menos en dicha entidad. Se puede observar por ejemplo que los encuestados conf√≠an menos en Partidos Pol√≠ticos, Sindicatos, Justicia y Congreso.</p>
<p>Ahora vamos a proceder a hacer un an√°lisis factorial. Sin embargo, antes de realizar este an√°lisis, se debe hacer una prueba de adecuaci√≥n, que b√°sicamente responde a la pregunta ¬øPodemos encontrar factores en nuestro conjunto de datos? Existen dos m√©todos (quiz√°s m√°s) para verificar la adecuaci√≥n de la muestra de datos para un an√°lisis factorial:</p>
<ul>
<li>Prueba de Bartlett</li>
<li>Prueba de Kaiser-Meyer-Olkin (desde ahora KMO)</li>
</ul>
<p>La prueba de Bartlett es una prueba de hip√≥tesis que verifica si existe correlaci√≥n entre las variables, y lo que hace es comparar la matriz de correlaci√≥n de la muestra con una matriz identidad (es decir, que no haya correlaci√≥n). Si la diferencia no es significativa, entonces no deber√≠amos aplicar an√°lisis factorial.</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">factor_analyzer</span> <span class="k">as</span> <span class="n">fact</span>


<span class="n">chisq</span><span class="p">,</span> <span class="n">pvalue</span> <span class="o">=</span> <span class="n">fact</span><span class="p">.</span><span class="n">calculate_bartlett_sphericity</span><span class="p">(</span><span class="n">trust_df</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Chi-Cuadrado: </span><span class="si">{</span><span class="n">chisq</span><span class="si">}</span><span class="s">, p-value: </span><span class="si">{</span><span class="n">pvalue</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>
<pre><code>Chi-Cuadrado: 2897.0676232781584, p-value: 0.0
</code></pre>
<p>En este caso, el <code>p-value</code> es 0, por lo que podemos rechazar la hip√≥tesis de que no hay correlaci√≥n en los datos.</p>
<p>La prueba de <strong>Kaiser-Meyer-Olkin (KMO)</strong> miden la idoneidad de los datos para un an√°lisis factorial. Determina la adecuaci√≥n para cada variable observada y para el modelo completo. La prueba de KMO estima la proporci√≥n de varianza entre todas las variables observadas. Los valores de KMO est√°n entre 0 y 1. Un valor de KMO menor que 0.6 se considera inadecuado:</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">kmo_all</span><span class="p">,</span> <span class="n">kmo_model</span> <span class="o">=</span> <span class="n">fact</span><span class="p">.</span><span class="n">calculate_kmo</span><span class="p">(</span><span class="n">trust_df</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Valor KMO para el modelo: </span><span class="si">{</span><span class="n">kmo_model</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>
<pre><code>Valor KMO para el modelo: 0.8299274694302806
</code></pre>
<p>En este caso obtenemos un valor de 0.83, lo que cumple el requisito para que el an√°lisis factorial sea adecuado. Ahora procederemos a realizar el an√°lisis factorial. Para escoger la cantidad de componentes, por lo general se hace un <em>scree plot</em> que b√°sicamente grafica cada uno de los valores propios (b√°sicamente la varianza explicada por cada factor de la varianza total), y se escogen los valores propios que sean mayores que 1:</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">factorize</span> <span class="o">=</span> <span class="n">fact</span><span class="p">.</span><span class="n">FactorAnalyzer</span><span class="p">(</span><span class="n">rotation</span><span class="o">=</span><span class="s">"varimax"</span><span class="p">)</span>
<span class="n">factorize</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trust_df</span><span class="p">)</span>
<span class="n">factor_screeplot</span> <span class="o">=</span> <span class="n">factorize</span><span class="p">.</span><span class="n">get_eigenvalues</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">factor_screeplot</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">factor_screeplot</span><span class="p">,</span> <span class="s">"o-"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"tomato"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"N√∫mero de Factor"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Valores Propios"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Scree plot"</span><span class="p">)</span>
</code></pre></div></div>
<div align="center">
<p><img src="https://raw.githubusercontent.com/dpalmasan/homepage/master/public/imgs/scree_plot.png" alt="valprop"></p>
<p><em>Fig 7: Valores propios vs n√∫mero de factores.</em></p>
</div>
<p>De los resultados, podemos observar que podemos escoger 4 factores. Luego podemos ver las cargas de cada factor. Las cargas factoriales son b√°sicamente las relaciones de cada factor con cada variable:</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">factorize</span> <span class="o">=</span> <span class="n">fact</span><span class="p">.</span><span class="n">FactorAnalyzer</span><span class="p">(</span><span class="n">n_factors</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="s">"varimax"</span><span class="p">)</span>
<span class="n">factorize</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trust_df</span><span class="p">)</span>
<span class="n">factor_loadings</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">factorize</span><span class="p">.</span><span class="n">loadings_</span><span class="p">,</span>
    <span class="n">index</span><span class="o">=</span><span class="n">trust_df</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">(</span><span class="s">"Factor 1"</span><span class="p">,</span> <span class="s">"Factor 2"</span><span class="p">,</span> <span class="s">"Factor 3"</span><span class="p">,</span> <span class="s">"Factor 4"</span><span class="p">))</span>
<span class="n">factor_loadings</span>
</code></pre></div></div>
<div align="center">
<p><img src="https://raw.githubusercontent.com/dpalmasan/homepage/master/public/imgs/factores.png" alt="fact"></p>
</div>
<p>La matriz anterior es un poco complicada de interpretar. Por lo general, un criterio de corte para las cargas factoriales, es remover los factores cuya carga factorial sea menor que 0.4, haremos eso procesando los datos:</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">factor_loadings</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">factor_loadings</span> <span class="o">&gt;=</span> <span class="mf">0.4</span><span class="p">,</span> <span class="s">""</span><span class="p">)</span>
</code></pre></div></div>
<div align="center">
<p><img src="https://raw.githubusercontent.com/dpalmasan/homepage/master/public/imgs/factores_filtrados.png" alt="factfilt"></p>
</div>
<p>Observamos que el factor 4 fue descartado, debido a que el criterio de corte, consider√≥ que las cargas factoriales no estaban por sobre el umbral. En el resto de los factores, podemos hacer la siguiente interpretaci√≥n: El factor 1, corresponde a medidas relacionadas a como los encuestados ven al gobierno, y temas relacionados a la pol√≠tica. El factor 2 est√° relacionado con componentes de justicia, como las fuerzas armadas, justicia y carabineros. El factor 3 est√° relacionado a la confianza de la gente en los medios de prensa. Y las variables como iglesia Cat√≥lica, iglesia evang√©lica, sindicatos, empresas y universidades, no presentan carga significativa en ninguno de los factores. Probablemente por alta cantidad de datos incompletos, entre otras cosas.</p>
<p>Finalmente observamos que los tres factores  escogidos explican aproximadamente un 31% de la varianza en los datos:</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">np</span><span class="p">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">factorize</span><span class="p">.</span><span class="n">get_factor_variance</span><span class="p">()),</span>
    <span class="n">index</span><span class="o">=</span><span class="p">(</span><span class="s">"SS Loadings"</span><span class="p">,</span> <span class="s">"Proportion Var"</span><span class="p">,</span> <span class="s">"Cumulative Var"</span><span class="p">),</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">(</span><span class="s">"Factor 1"</span><span class="p">,</span> <span class="s">"Factor 2"</span><span class="p">,</span> <span class="s">"Factor 3"</span><span class="p">,</span> <span class="s">"Factor 4"</span><span class="p">))</span>
</code></pre></div></div>
<div align="center">
<p><img src="https://raw.githubusercontent.com/dpalmasan/homepage/master/public/imgs/factor_stats.png" alt="var"></p>
</div>
<p>Observamos que el cuarto factor tiene un <code>SS loadings</code> bajo. El <code>SS loading</code> es la suma cuadr√°tica de las cargas factoriales. Generalmente se conservan los factores cuya suma cuadr√°tica de cargas sea mayor que 1 (consistente con el criterio de corte, ya que vemos que el factor 4, tiene un valor menor que uno).</p>
<p>Finalmente, por completitud, si quieren transformar las observaciones a factores, tienen que usar el m√©todo <code>transform</code> como sigue:</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Nos dimos cuenta que solo son 3 factores los relevantes
</span><span class="n">factorize</span> <span class="o">=</span> <span class="n">fact</span><span class="p">.</span><span class="n">FactorAnalyzer</span><span class="p">(</span><span class="n">n_factors</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="s">"varimax"</span><span class="p">)</span>
<span class="n">factorize</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trust_df</span><span class="p">)</span>
<span class="n">transformed_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">factorize</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">trust_df</span><span class="p">),</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">(</span><span class="s">"Factor 1"</span><span class="p">,</span> <span class="s">"Factor 2"</span><span class="p">,</span> <span class="s">"Factor 3"</span><span class="p">,</span> <span class="s">"Factor 4"</span><span class="p">))</span>
<span class="n">transformed_df</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>
<div align="center">
<p><img src="https://raw.githubusercontent.com/dpalmasan/homepage/master/public/imgs/factores_transform.png" alt="trans"></p>
</div>
<h2 id="anlisis-de-componentes-principales">An√°lisis de Componentes Principales</h2>
<p>Primero, es interesante entender qu√© significa componentes principales, y cu√°l es la intuici√≥n. En t√©rminos simples, las componentes principales ser√≠an ejes en donde ocurre la m√°xima variaci√≥n en los datos. En t√©rminos simples, podr√≠an verse las componentes principales como un cambio de sistema de coordenadas, o un cambio de vectores bases. Por ejemplo, una transformaci√≥n de un sistema de coordenadas a otro, podr√≠a ser como sigue:</p>
<p>$$
x_1 \begin{bmatrix} a \\\ c \end{bmatrix} + x_2 \begin{bmatrix} b \\\ d \end{bmatrix} =
\begin{bmatrix} ax_1 + bx_2 \\\ cx_1 + dx_2 \end{bmatrix}
$$</p>
<p>En este caso, los vectores en el lado izquierdo son vectores base, como referencia:</p>
<ul>
<li>$[a, c]$ ; $[b, d]$ son los vectores base</li>
<li>En el sistema cartesiano convencional, los vectores base son $[1, 0]$; $[0, 1]$ (o comunmente $x$ e $y$)</li>
</ul>
<p>No cualquier vector puede ser un vector base, algunos puntos clave:</p>
<ul>
<li>Los vectores base son los mismos para todos los registros de un conjunto de datos.</li>
<li>Los vectores base son ortonormales, es decir, perpendiculares entre s√≠ y con norma 1 (largo 1)</li>
<li>Finalmente, podemos representar cada registro del conjunto de datos como una combinaci√≥n lineal de sus vectores base.</li>
</ul>
<p>Lo anterior puede verse abstracto, por lo que tomemos un ejemplo:</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">draw_vector</span><span class="p">(</span><span class="n">v0</span><span class="p">,</span> <span class="n">v1</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span> <span class="ow">or</span> <span class="n">plt</span><span class="p">.</span><span class="n">gca</span><span class="p">()</span>
    <span class="n">arrowprops</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">arrowstyle</span><span class="o">=</span><span class="s">"-&gt;"</span><span class="p">,</span>
        <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">shrinkA</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shrinkB</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="s">"b"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">annotate</span><span class="p">(</span><span class="s">""</span><span class="p">,</span> <span class="n">v1</span><span class="p">,</span> <span class="n">v0</span><span class="p">,</span> <span class="n">arrowprops</span><span class="o">=</span><span class="n">arrowprops</span><span class="p">)</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">rng</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">rng</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">200</span><span class="p">)).</span><span class="n">T</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>


<span class="n">pca</span> <span class="o">=</span> <span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pca</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>


<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s">"ro"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">length</span><span class="p">,</span> <span class="n">vector</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">pca</span><span class="p">.</span><span class="n">explained_variance_</span><span class="p">,</span> <span class="n">pca</span><span class="p">.</span><span class="n">components_</span><span class="p">):</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">vector</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">length</span><span class="p">)</span>
    <span class="n">draw_vector</span><span class="p">(</span><span class="n">pca</span><span class="p">.</span><span class="n">mean_</span><span class="p">,</span> <span class="n">pca</span><span class="p">.</span><span class="n">mean_</span> <span class="o">+</span> <span class="n">v</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Ilustraci√≥n PCA"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"$x_1$"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"$x_2$"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">axis</span><span class="p">(</span><span class="s">"equal"</span><span class="p">)</span>
</code></pre></div></div>
<div align="center">
<p><img src="https://raw.githubusercontent.com/dpalmasan/homepage/master/public/imgs/pca_ex1.png" alt="pca"></p>
<p><em>Fig 8: Ilustraci√≥n de PCA.</em></p>
</div>
<p>Como se observa en la figura, el conjunto de datos llevarse a otro sistema de coordenadas considerando estos vectores base (para transformar los datos, basta simplemente con aplicar la transformaci√≥n lineal descrita). Podemos ver tambi√©n que cada eje se escoge en la direcci√≥n donde haya mayor variabilidad en los datos (varianza).</p>
<p>Un ejemplo de aplicaci√≥n de las componentes principales, es en el caso de reducci√≥n dimensional. Por ejemplo, podemos eliminar las componentes con menor cantidad de varianza, es decir, que explican menos la variabilidad en los datos, y en consecuencia, estar√≠amos proyectando el espacio dimensional en un espacio de menos dimensiones. Para ilustrar esto, consideremos el siguiente ejemplo:</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">mpl_toolkits.mplot3d</span> <span class="kn">import</span> <span class="n">Axes3D</span>


<span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">"figure.figsize"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="n">EJEMPLO_PCA_URL</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s">"https://gist.githubusercontent.com/dpalmasan/"</span>
    <span class="s">"1bba35979f1f284ddf7c8c540f60c66f/raw/"</span>
    <span class="s">"4a3fcf2f97fa2d85be69eedaa98b0ed6f46a3017/tetra.csv"</span>
<span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">EJEMPLO_PCA_URL</span><span class="p">)</span>

<span class="n">pca</span> <span class="o">=</span> <span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pca</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s">"3d"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">"x"</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s">"y"</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s">"z"</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s">"b"</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">"o"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"X"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"Y"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_zlabel</span><span class="p">(</span><span class="s">"Z"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Tetera en 3D"</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s">"ro"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">"PC 1"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">"PC 2"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Proyecci√≥n en 2D de la tetera (sombra)"</span><span class="p">)</span>
</code></pre></div></div>
<div align="center">
<p><img src="https://raw.githubusercontent.com/dpalmasan/homepage/master/public/imgs/intuicion_pca.png" alt="pca"></p>
<p><em>Fig 9: Intuici√≥n PCA.</em></p>
</div>
<p>En el ejemplo de la tetera, b√°sicamente tenemos un espacio dimensional de 3 dimensiones (<code>x, y, z</code>). Cuando aplicamos <code>PCA</code> y eliminamos una componente (la de menos variaci√≥n), b√°sicamente estamos calculando una proyecci√≥n de este espacio a un espacio de menor dimensi√≥n, intentando mantener la variabilidad del espacio original. Intuitivamente, en este caso particular, podr√≠a pensarse en cada componente principal como visualizar la ‚Äúsombra‚Äù de la tetera. Claro, que no estamos restringidos s√≥lo a espacios de 3 dimensiones, si no que tambi√©n podemos reducir espacios de cualquier dimension a uno de menor dimensionalidad, por ejemplo, para visualizar datos.</p>
<p>Consideremos los datos de <a href="https://github.com/datasets/world-wealth-and-income-database" target="_blank" rel="noopener noreferrer"><em>World Wealth and Income Database</em></a>.</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>


<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"income-db.csv"</span><span class="p">)</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span><span class="p">[</span><span class="s">"object"</span><span class="p">])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pca</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Transformar a dos dimensiones
</span><span class="n">X</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">income_greater</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">"income"</span><span class="p">]</span> <span class="o">==</span> <span class="s">"&gt;50K"</span>
<span class="n">income_leq</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">"income"</span><span class="p">]</span> <span class="o">==</span> <span class="s">"&lt;=50K"</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">income_greater</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">income_greater</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s">"ro"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">income_leq</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">income_leq</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s">"bo"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">((</span><span class="s">"income &gt; 50K"</span><span class="p">,</span> <span class="s">"income &lt;= 50K"</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"componente principal 1"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Componente principal 2"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Proyecci√≥n datos num√©ricos con PCA"</span><span class="p">)</span>
</code></pre></div></div>
<div align="center">
<p><img src="https://raw.githubusercontent.com/dpalmasan/homepage/master/public/imgs/prueba_ejemplo.png" alt="ejpca"></p>
<p><em>Fig 10: Ejemplo PCA.</em></p>
</div>
<p>Adem√°s, podemos ver c√≥mo impacta cada variable a cada nueva dimensi√≥n:</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># El signo no importa, importa el signo relativo, podemos ver
# C√≥mo afecta cada componente a cada variable por ejemplo
</span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">pca</span><span class="p">.</span><span class="n">components_</span><span class="p">,</span>
    <span class="n">columns</span><span class="o">=</span><span class="n">df</span><span class="p">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span><span class="p">[</span><span class="s">"object"</span><span class="p">]).</span><span class="n">columns</span><span class="p">,</span>
    <span class="n">index</span> <span class="o">=</span> <span class="p">[</span><span class="s">"PC-1"</span><span class="p">,</span> <span class="s">"PC-2"</span><span class="p">])</span>
</code></pre></div></div>
<div align="center">
<p><img src="https://raw.githubusercontent.com/dpalmasan/homepage/master/public/imgs/pca_corr.png" alt="pccomp"></p>
</div>
<p>Debe considerarse tambi√©n, que usualmente se estandarizan los datos antes de aplicar PCA (por ejemplo normalizar, o llevar a una misma escala), por lo tanto, cuando lo apliquen, hagan este pre-procesamiento antes. Finalmente, y recapitulando hasta ahora, PCA es un maximizador de varianza, que proyecta los datos originales en las direcciones donde la varianza es m√°xima.</p>
<h3 id="ejemplo-datos-mnist-dgitos-manuscritos">Ejemplo Datos MNIST (d√≠gitos manuscritos)</h3>
<p>Finalmente, por completitud, haremos el ejemplo t√≠pico de analizar el <a href="https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits" target="_blank" rel="noopener noreferrer">conjunto de datos de reconocimiento de d√≠gitos</a>, que en esencia consiste en im√°genes de <code>8x8</code> donde cada imagen contiene un d√≠gito manuscrito.</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>


<span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">digits</span><span class="p">.</span><span class="n">keys</span><span class="p">())</span>
<span class="k">print</span><span class="p">(</span><span class="n">digits</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">digits</span><span class="p">.</span><span class="n">target</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">digits</span><span class="p">.</span><span class="n">feature_names</span><span class="p">)</span>
</code></pre></div></div>
<p>Por ejemplo, miremos un d√≠gito arbitrario:</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>


<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">plt</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="n">use</span><span class="p">(</span><span class="s">"seaborn"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">"figure.figsize"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">"figure.dpi"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">200</span>

<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">digits</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">reshape</span><span class="p">((</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">((</span><span class="sa">f</span><span class="s">"Imagen de </span><span class="si">{</span><span class="n">digits</span><span class="p">.</span><span class="n">target</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s">"</span><span class="p">))</span>
</code></pre></div></div>
<div align="center">
<p><img src="https://raw.githubusercontent.com/dpalmasan/homepage/master/public/imgs/digito.png" alt="mnistsample"></p>
<p><em>Fig 11: Muestra conjunto de datos de d√≠gitos.</em></p>
</div>
<p>Como los datos son de im√°genes de <code>8x8</code> p√≠xeles, b√°sicamente cada atributo es el valor del p√≠xel (en escala de grises), por lo tanto se tienen 64 atr√≠butos por registros. Dada la naturaleza del problema, <code>PCA</code> pareciera ser una buena opci√≥n para visualizar atributos similares en los d√≠gitos (por ejemplo curvatura, simetr√≠a, etc.):</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>


<span class="c1"># Contrario a lo que dice la lectura, PCA NO llama a StandardScaler por debajo
# Lo que s√≠ hace es centrar los datos pero NO los escala
# En este caso da igual, porque todos los atributos est√°n en la misma escala
</span><span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">digits</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Dimensionalidad original: </span><span class="si">{</span><span class="n">digits</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Dimensionalidad despu√©s de PCA: </span><span class="si">{</span><span class="n">X_pca</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>
<p>En este caso, transformaremos a dos componentes, para hacer una visualizaci√≥n en 2D y dar un vistazo a si existe alguna relaci√≥n en los registros:</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="n">edgecolor</span><span class="o">=</span><span class="s">"none"</span><span class="p">,</span>
    <span class="n">c</span><span class="o">=</span><span class="n">digits</span><span class="p">.</span><span class="n">target</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">cmap</span><span class="o">=</span><span class="s">"Set1"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">colorbar</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"PC 1"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"PC 2"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"D√≠gitos proyectados a dos dimensiones"</span><span class="p">)</span>
</code></pre></div></div>
<div align="center">
<p><img src="https://raw.githubusercontent.com/dpalmasan/homepage/master/public/imgs/pc_digits.png" alt="proj"></p>
<p><em>Fig 12: Im√°genes de d√≠gitos manuscritos proyectados en un espacio bi-dimensional.</em></p>
</div>
<p>De la figura se pueden desprender algunas relaciones interesantes. Por ejemplo, el <code>4</code> est√° cerca del <code>9</code>, probablemente porque tienen formas similares, lo mismo el <code>3</code> con el <code>8</code>. Tambi√©n vemos que en general los d√≠gitos est√°n agrupados en distintas porciones del espacio.</p>
<p>Para elegir la cantidad de componentes, en general se debe tener un umbral de cu√°nta informaci√≥n de los datos se quiere retener, o en t√©rminos matem√°ticos, cu√°nta varianza explicada en los datos se quiere considerar. Para ello podemos hacer el siguiente gr√°fico:</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pca_full</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">().</span><span class="n">fit</span><span class="p">(</span><span class="n">digits</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">pca_full</span><span class="p">.</span><span class="n">n_components_</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">pca_full</span><span class="p">.</span><span class="n">explained_variance_ratio_</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s">"Varianza por componente"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">pca_full</span><span class="p">.</span><span class="n">components_</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">pca_full</span><span class="p">.</span><span class="n">explained_variance_ratio_</span><span class="p">),</span>
<span class="n">color</span><span class="o">=</span><span class="s">"tomato"</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Varianza acumulada"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Cantidad de Dimensiones"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Varianza"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
</code></pre></div></div>
<div align="center">
<p><img src="https://raw.githubusercontent.com/dpalmasan/homepage/master/public/imgs/cumulative_var.png" alt="mnistsample"></p>
<p><em>Fig 13: Varianza acumulada respecto a la cantidad de dimensiones.</em></p>
</div>
<p>En este caso, podemos ver que alrededor de 10 componentes deber√≠a ser suficiente para explicar gran cantidad de la varianza en los datos (entre 0.7 y 0.8). Esto tambi√©n se puede usar como ‚Äúfiltro‚Äù, ya que quiz√°s, mayores componentes est√©n ajustandose al ruido en los datos. Finalmente, visualicemos c√≥mo contribuye cada componente a cada d√≠gito:</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">show</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">imshape</span><span class="p">,</span> <span class="n">fontsize</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="s">"""
    Funci√≥n auxiliar para agregar gr√°ficos a la figura.
    """</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="n">grid</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">xticks</span><span class="o">=</span><span class="p">[],</span> <span class="n">yticks</span><span class="o">=</span><span class="p">[])</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">imshape</span><span class="p">),</span> <span class="n">interpolation</span><span class="o">=</span><span class="s">"nearest"</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s">"Blues"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">title</span><span class="p">:</span>
        <span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">fontsize</span><span class="p">)</span>



<span class="k">def</span> <span class="nf">plot_pca_components</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">coefs</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cmps</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                        <span class="n">imshape</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span>
                        <span class="n">show_mean</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="s">"""
    Graficar componentes PCA para dataset de d√≠gitos.
    """</span>
    <span class="k">if</span> <span class="n">coefs</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">coefs</span> <span class="o">=</span> <span class="n">x</span>

    <span class="k">if</span> <span class="n">cmps</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">cmps</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">coefs</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="c1"># Como datos fueron centrados en 0, para reconstruirlos en el espacio
</span>    <span class="c1"># Original, a cada componente se le agrega el promedio
</span>    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">mean</span>

    <span class="c1"># Ajustar ancho y alto de figura
</span>    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">1.2</span> <span class="o">*</span> <span class="p">(</span><span class="mi">5</span> <span class="o">+</span> <span class="n">n_components</span><span class="p">),</span> <span class="mf">1.2</span> <span class="o">*</span> <span class="mi">2</span><span class="p">))</span>

    <span class="c1"># Crear distribuci√≥n de figuras dentro del gr√°fico
</span>    <span class="n">grid</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">GridSpec</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span> <span class="o">+</span> <span class="nb">int</span><span class="p">(</span><span class="n">show_mean</span><span class="p">)</span> <span class="o">+</span> <span class="n">n_components</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

    <span class="c1"># Se grafica en las dos primeras filas y dos primeras columnas del plot
</span>    <span class="n">show</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="nb">slice</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="nb">slice</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">x</span><span class="p">,</span> <span class="n">imshape</span><span class="p">,</span> <span class="n">fontsize</span><span class="p">,</span> <span class="s">"Original"</span><span class="p">)</span>
    <span class="n">approx</span> <span class="o">=</span> <span class="n">mean</span>
    <span class="n">counter</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="k">if</span> <span class="n">show_mean</span><span class="p">:</span>
        <span class="n">show</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">mean</span><span class="p">,</span> <span class="n">imshape</span><span class="p">,</span> <span class="n">fontsize</span><span class="p">,</span> <span class="sa">r</span><span class="s">"$\mu$"</span><span class="p">)</span>
        <span class="n">show</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">approx</span><span class="p">,</span> <span class="n">imshape</span><span class="p">,</span> <span class="n">fontsize</span><span class="p">,</span> <span class="sa">r</span><span class="s">"$1 \cdot \mu$"</span><span class="p">)</span>
        <span class="n">counter</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_components</span><span class="p">):</span>
        <span class="c1"># Reconstruir imagen considerando i + 1 componentes componentes
</span>        <span class="n">approx</span> <span class="o">=</span> <span class="n">approx</span> <span class="o">+</span> <span class="n">coefs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">cmps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">show</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="n">counter</span><span class="p">,</span> <span class="n">cmps</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">imshape</span><span class="p">,</span>
            <span class="n">fontsize</span><span class="p">,</span> <span class="sa">r</span><span class="s">"$c_{0}$"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">show</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="n">counter</span><span class="p">,</span> <span class="n">approx</span><span class="p">,</span> <span class="n">imshape</span><span class="p">,</span>
            <span class="n">fontsize</span><span class="p">,</span> <span class="sa">r</span><span class="s">"$ {0:.2f} \cdot c_{1}$"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">coefs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">show_mean</span> <span class="ow">or</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">plt</span><span class="p">.</span><span class="n">gca</span><span class="p">().</span><span class="n">text</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">,</span> <span class="s">"$+$"</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s">"right"</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s">"bottom"</span><span class="p">,</span>
                          <span class="n">transform</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="n">gca</span><span class="p">().</span><span class="n">transAxes</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="n">fontsize</span><span class="p">)</span>

    <span class="n">show</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="nb">slice</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="nb">slice</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="bp">None</span><span class="p">),</span> <span class="n">approx</span><span class="p">,</span> <span class="n">imshape</span><span class="p">,</span> <span class="n">fontsize</span><span class="p">,</span> <span class="s">"Aproximaci√≥n"</span><span class="p">)</span>


<span class="n">pca_10</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">10</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">digits</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">X_pca10</span> <span class="o">=</span> <span class="n">pca_10</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">digits</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>
<span class="n">plot_pca_components</span><span class="p">(</span><span class="n">digits</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="mi">6</span><span class="p">],</span> <span class="n">X_pca10</span><span class="p">[</span><span class="mi">6</span><span class="p">],</span> <span class="n">pca_10</span><span class="p">.</span><span class="n">mean_</span><span class="p">,</span> <span class="n">pca_10</span><span class="p">.</span><span class="n">components_</span><span class="p">)</span>
</code></pre></div></div>
<div align="center">
<p><img src="https://raw.githubusercontent.com/dpalmasan/homepage/master/public/imgs/components_digits.png" alt="comp"></p>
<p><em>Fig 14: Ejemplo de diferentes componentes para un d√≠gito.</em></p>
</div>
<h1 id="agrupacin">Agrupaci√≥n</h1>
<p>Existen diversos escenarios en los cuales nos gustar√≠a agrupar los datos o encontrar grupos de datos, pues ello nos permitir√≠a encontrar informaci√≥n relevante acerca de la poblaci√≥n de datos de inter√©s. Algunos ejemplos:</p>
<ul>
<li>Segmentaci√≥n de clientes.</li>
<li>Sistemas de recomendaci√≥n.</li>
<li>Categorizaci√≥n de diversos grupos.</li>
<li>Segmentaci√≥n de Im√°genes.</li>
<li>Entre otros.</li>
</ul>
<p>Existen diversos m√©todos para agrupar datos, nosotros veremos uno bastante simple, que a√∫n a pesar de su simpleza, se utiliza en la pr√°cticas. El algorimo que veremos es conocido como <strong>K-means</strong>.</p>
<h2 id="clstering-k-means">Cl√∫stering K-Means</h2>
<p>La t√©cnica de cl√∫stering consiste en dividir los datos en diferentes grupos, donde los registros en cada grupo son similares entre s√≠. Un objetivo del cl√∫stering es encontrar grupos interesantes de datos. Estos grupos pueden ser utilizados directamente, analizados en profunidad, o ser usados como atributos en un algoritmo de clasificaci√≥n o de regresi√≥n.</p>
<p>El algoritmo <code>K-means</code> divide los datos en <code>K</code> cl√∫sters mediante la minimizaci√≥n de la suma de las distancias cuadr√°ticas de cada registro al centro de su cl√∫ster asignado. En general la serie de pasos a seguir es la siguiente:</p>
<ol>
<li>Comenzar con <code>K</code> centros aleatorios.</li>
<li>Asignar cada registro a un cl√∫ster en base a su distancia hacia el centro. Se asigna al cl√∫ster cuya distancia sea la m√≠nima respecto al centro.</li>
<li>Luego, calcular el ‚Äúcentro de masa‚Äù de cada cl√∫ster (recalcular centros)</li>
<li>Volver al paso 2, y repetir hasta satisfacer un criterio de detenci√≥n (por ejemplo que asignaci√≥n no cambie entre iteraciones).</li>
</ol>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">kmeans_clustering</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">clusters</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">maxit</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="s">"""Calcula cl√∫sters usando K-means.

    Este c√≥digo lo hice cuando era estudiante as√≠ que est√° feo, me
    disculpo por eso.

    :param X: Conjunto de datos
    :type X: np.array
    :param K: Cantidad de cl√∫sters, defaults to 5
    :type K: int, optional
    :param maxit: Cantidad m√°xima de iteraciones, defaults to 10
    :type maxit: int, optional
    :return: (cluster_assign, centroides, iteraciones)
    :rtype: tuple(np.array, np.array, int)
    """</span>
    <span class="c1"># Sample Size
</span>    <span class="n">N</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Inicializar vector de cl√∫sters
</span>    <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>

    <span class="c1"># Inicializar centroides, se escogen al azar datos del conjunto de datos
</span>    <span class="n">mu</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">clusters</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span> <span class="p">:]</span>

    <span class="c1"># Asignar datos a cada cl√∫ster
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="n">kmin</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="n">min_dist</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s">"Inf"</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">clusters</span><span class="p">):</span>
            <span class="n">dist</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">mu</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="p">:])</span>
            <span class="k">if</span> <span class="n">dist</span> <span class="o">&lt;</span> <span class="n">min_dist</span><span class="p">:</span>
                <span class="n">min_dist</span> <span class="o">=</span> <span class="n">dist</span>
                <span class="n">kmin</span> <span class="o">=</span> <span class="n">k</span>
        <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">kmin</span> <span class="o">+</span> <span class="mi">1</span>


    <span class="n">c_new</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="n">it</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="c1"># Iterar hasta m√°ximo de iteraciones o hasta que no haya cambios
</span>    <span class="c1"># en la asignaci√≥n de cl√∫sters
</span>    <span class="k">while</span> <span class="n">it</span> <span class="o">&lt;=</span> <span class="n">maxit</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">c</span> <span class="o">==</span> <span class="n">c_new</span><span class="p">):</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">c_new</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
            <span class="n">kmin</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">min_dist</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s">"Inf"</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">clusters</span><span class="p">):</span>
                <span class="n">dist</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">norm</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">mu</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="p">:])</span>
                <span class="k">if</span> <span class="n">dist</span> <span class="o">&lt;</span> <span class="n">min_dist</span><span class="p">:</span>
                    <span class="n">min_dist</span> <span class="o">=</span> <span class="n">dist</span>
                    <span class="n">kmin</span> <span class="o">=</span> <span class="n">k</span>

            <span class="n">c_new</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">kmin</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="c1"># Actualizar centroides a "Centro de Masa"
</span>        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">clusters</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">Xk</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">c_new</span> <span class="o">==</span> <span class="n">k</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">mu</span><span class="p">[</span><span class="n">k</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span>  <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">Xk</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">Xk</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">it</span><span class="p">)</span>
</code></pre></div></div>
<p>Probemos con el m√≠tico conjunto de datos <a href="https://archive.ics.uci.edu/ml/datasets/iris" target="_blank" rel="noopener noreferrer"><em>Iris</em></a>. Este conjunto de datos b√°sicamente consiste en muestras de distintas plantas iris, donde los atributos medidos son b√°sicamente longitud y ancho de los s√©palos y p√©talos:</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>


<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">plt</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="n">use</span><span class="p">(</span><span class="s">"seaborn"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">"figure.figsize"</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">"figure.dpi"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">200</span>

<span class="n">iris_data</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">iris_data</span><span class="p">.</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">iris_data</span><span class="p">.</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s">"none"</span><span class="p">,</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">"k"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">iris_data</span><span class="p">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">iris_data</span><span class="p">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</code></pre></div></div>
<div align="center">
<p><img src="https://raw.githubusercontent.com/dpalmasan/homepage/master/public/imgs/iris_data_unlabeled.png" alt="comp"></p>
<p><em>Fig 15: Muestra de datos para el conjunto de datos Iris.</em></p>
</div>
<p>Si aplicaramos el algoritmo descrito con <code>K = 3</code>, ocurrir√≠a lo siguiente:</p>
<div align="center">
<p><img src="https://raw.githubusercontent.com/dpalmasan/homepage/master/public/imgs/kmeans.gif" alt="comp"></p>
<p><em>Fig 16: KMeans en acci√≥n.</em></p>
</div>
<p>Ahora intentemos darle una interpretaci√≥n a cada cl√∫ster. Consideremos las clases de plantas iris en el conjunto de datos. Para obtener los cl√∫sters, utilizaremos la implementaci√≥n de <code>sklearn</code>:</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>


<span class="c1"># Configuramos 3 cl√∫sters, para seguir el ejemplo
</span><span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">iris_data</span><span class="p">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)])</span>

<span class="n">c1</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span> <span class="o">==</span> <span class="mi">0</span>
<span class="n">c2</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span> <span class="o">==</span> <span class="mi">1</span>
<span class="n">c3</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span> <span class="o">==</span> <span class="mi">2</span>

<span class="n">setosa</span> <span class="o">=</span> <span class="n">iris_data</span><span class="p">.</span><span class="n">target</span> <span class="o">==</span> <span class="mi">0</span>
<span class="n">versicolor</span> <span class="o">=</span> <span class="n">iris_data</span><span class="p">.</span><span class="n">target</span> <span class="o">==</span> <span class="mi">1</span>
<span class="n">virginica</span> <span class="o">=</span> <span class="n">iris_data</span><span class="p">.</span><span class="n">target</span> <span class="o">==</span> <span class="mi">2</span>

<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">121</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">iris_data</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">setosa</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">iris_data</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">setosa</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s">"none"</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">"r"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">iris_data</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">versicolor</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">iris_data</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">versicolor</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s">"none"</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">"g"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">iris_data</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">virginica</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">iris_data</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">virginica</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s">"none"</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">"b"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">iris_data</span><span class="p">.</span><span class="n">target_names</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">iris_data</span><span class="p">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">iris_data</span><span class="p">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

<span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">122</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">iris_data</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">c1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">iris_data</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">c1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s">"none"</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">"m"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">iris_data</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">c2</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">iris_data</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">c2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s">"none"</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">"c"</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">iris_data</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">c3</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">iris_data</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="n">c3</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s">"none"</span><span class="p">,</span>
            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">"y"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">((</span><span class="s">"cl√∫ster 1"</span><span class="p">,</span> <span class="s">"cl√∫ster 2"</span><span class="p">,</span> <span class="s">"cl√∫ster 3"</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">iris_data</span><span class="p">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="n">iris_data</span><span class="p">.</span><span class="n">feature_names</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</code></pre></div></div>
<div align="center">
<p><img src="https://raw.githubusercontent.com/dpalmasan/homepage/master/public/imgs/clusters_int.png" alt="comp"></p>
<p><em>Fig 17: Grupos originales vs Cl√∫sters encontrados por KMeans.</em></p>
</div>
<p>En este caso el cl√∫ster 1 se puede interpretar como las plantas de clase <code>setosa</code>, el cl√∫ster 2 como plantas de clase <code>virginica</code> y el cl√∫ster 3 como plantas de clase <code>versicolor</code>.</p>
<p>Observaci√≥n: En este caso no lo hicimos, ya que los atributos se encontraban en escalas similares, pero, por lo general, al trabajar con cl√∫stering, se prefiere escalar los datos, para que no haya un atributo que tenga prioridad sobre otros. Pregunta para pensar ¬øQu√© pasa cuando consideramos dimensionalidades altas (o a medida que aumentamos la dimensionalidad)?</p>
<p>Otro problema que vemos es que el valor <code>K</code> de la cantidad de cl√∫sters es una entrada al algoritmo. Existen m√©todos para escoger la cantidad de cl√∫sters, algunas veces funcionan otras no. Existen otras formas estad√≠sticas tambi√©n para encontrar la cantidad de cl√∫sters, sin embargo, siempre hay que tener en cuenta el contexto <em>¬ømejor considerando qu√©?</em>. Una forma de encontrar la cantidad de cl√∫sters es utilizando el <em>m√©todo del codo</em>, en el cual corremos varias veces el algoritmo variando la cantidad de cl√∫sters y vemos como var√≠a la <strong>inercia</strong> de los cl√∫sters (b√°sicamente la suma cuadr√°tica de las distancias de cada centroide a cada registro que pertenece al cl√∫ster). Escogemos la cantidad de cl√∫sters hasta que la variaci√≥n en la inercia sea casi despreciable (en el gr√°fico se ve como un codo). Probemos esto para el ejemplo:</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">N</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">inertia</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">n_clusters</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">clusters</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">n_clusters</span><span class="p">):</span>
    <span class="n">inertia</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span>
        <span class="n">n_clusters</span><span class="o">=</span><span class="n">clusters</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="mi">1234</span><span class="p">).</span><span class="n">fit</span><span class="p">(</span><span class="n">iris_data</span><span class="p">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)]).</span><span class="n">inertia_</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">n_clusters</span><span class="p">,</span> <span class="n">inertia</span><span class="p">,</span> <span class="s">"o-"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"tomato"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Cantidad de clusters"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Inercia"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Elbow graph"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</code></pre></div></div>
<div align="center">
<p><img src="https://raw.githubusercontent.com/dpalmasan/homepage/master/public/imgs/elbow_graph.png" alt="elbow"></p>
<p><em>Fig 18: Gr√°fico de codo.</em></p>
</div>
<p>Para este caso particular, podemos observar que <code>K = 3</code> es un buen valor para la cantidad de cl√∫sters.</p>
<p>Existen otros algoritmos que no requieren conocer la cantidad de cl√∫sters apriori (ejemplo: <code>DBScan</code>).</p>
<h3 id="ejemplo-de-compresin-de-imgenes">Ejemplo de compresi√≥n de im√°genes</h3>
<p>Como √∫ltimo ejemplo de <code>Kmeans</code>, utilic√©moslo para comprimir una imagen. Lo que haremos ser√° hacer cl√∫stering, y generar super-p√≠xeles, que ser√°n grupos de pixeles, donde su valor de color ser√° el centroide del cl√∫ster. Para el ejemplo, comprimiremos la imagen para que utilice 30 colores, pero ah√≠ pueden ir jugando, teniendo la intuici√≥n de que reducir la cantidad de colores, reducir√° la calidad de la compresi√≥n:</p>
<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>


<span class="n">img</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">imread</span><span class="p">(</span><span class="s">"semana7/oso.jpg"</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">img</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">kmeans</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="c1"># Usar centroides para comprimir imagen
</span><span class="n">X_compressed</span> <span class="o">=</span> <span class="n">kmeans</span><span class="p">.</span><span class="n">cluster_centers_</span><span class="p">[</span><span class="n">kmeans</span><span class="p">.</span><span class="n">labels_</span><span class="p">]</span>
<span class="n">X_compressed</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">clip</span><span class="p">(</span><span class="n">X_compressed</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="s">"uint8"</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">)</span>

<span class="c1"># Re-escalar a dimensiones de imagen original
</span><span class="n">X_compressed</span> <span class="o">=</span> <span class="n">X_compressed</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Im√°gen original"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">axis</span><span class="p">(</span><span class="s">"off"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">imshow</span><span class="p">(</span><span class="n">X_compressed</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Imagen comprimida con 30 colores"</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">axis</span><span class="p">(</span><span class="s">"off"</span><span class="p">)</span>
</code></pre></div></div>
<div align="center">
<p><img src="https://raw.githubusercontent.com/dpalmasan/homepage/master/public/imgs/oso_compresion.png" alt="oso1">
<img src="https://raw.githubusercontent.com/dpalmasan/homepage/master/public/imgs/oso_compresion2.png" alt="oso2">
<img src="https://raw.githubusercontent.com/dpalmasan/homepage/master/public/imgs/oso_compresion3.png" alt="oso3"></p>
<p><em>Fig 19: Compresi√≥n de im√°genes considerando <code>K = 30</code> <code>K = 10</code> y <code>K = 5</code>.</em></p>
</div>
<h1 id="conclusiones">Conclusiones</h1>
<ul>
<li>Se introdujo el concepto de aprendizaje ‚Äúno supervisado‚Äù y algunos ejemplos como reducci√≥n dimensional y agrupamiento.</li>
<li>Se habl√≥ de los problemas que pueden surgir cuando se aumenta la dimensionalidad y se habl√≥ sobre la maldici√≥n de la dimensionalidad, donde se dieron algunas intuiciones.</li>
<li>Se revisaron t√©cnicas t√≠picas de reducci√≥n dimensional tales como an√°lisis factorial y an√°lisis de componentes principales y se mostraron ejemplos pr√°cticos.</li>
<li>Se introdujo un ejemplo de agrupamiento (cl√∫stering), se explic√≥ did√°cticamente en qu√© consiste el algoritmo <code>KMeans</code> y se mostr√≥ un ejemplo pr√°ctico de compresi√≥n de im√°genes.</li>
</ul>

  </div>
<div id="disqus_thread"></div>
  <script>
    var disqus_config = function () {
      this.page.url = '';
      this.page.identifier = 'https://dpalmasan.github.io/website/probability/algorithms/ai/2024/03/08/mas-reciclaje.html';
    };

    (function() {
      var d = document, s = d.createElement('script');

      s.src = 'https://dpalmasan.disqus.com/embed.js';

      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow noopener noreferrer" target="_blank">comments powered by Disqus.</a>
</noscript>
<a class="u-url" href="/website/probability/algorithms/ai/2024/03/08/mas-reciclaje.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/website/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="https://dpalmasan.github.io/website/feed.xml">
            <svg class="svg-icon orange">
              <path d="M12.8 16C12.8 8.978 7.022 3.2 0 3.2V0c8.777 0 16 7.223 16 16h-3.2zM2.194
                11.61c1.21 0 2.195.985 2.195 2.196 0 1.21-.99 2.194-2.2 2.194C.98 16 0 15.017 0
                13.806c0-1.21.983-2.195 2.194-2.195zM10.606
                16h-3.11c0-4.113-3.383-7.497-7.496-7.497v-3.11c5.818 0 10.606 4.79 10.606 10.607z"></path>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">dpalmasan</li>
          
        </ul>
      </div>
      <div class="footer-col">
        <p>Este es un blog donde compartir√© un poco sobre m√≠ y mis experiencias en el mundo tecnol√≥gico...</p>
      </div>
    </div>

    <div class="social-links">
<ul class="social-media-list">
<li>
    <a rel="me noopener noreferrer" href="https://www.linkedin.com/in/dpalmasan/" target="_blank" title="Mi perfil en Linkedin">
      <span class="grey fa-brands fa-linkedin fa-lg"></span>
    </a>
  </li>
<li>
    <a rel="me noopener noreferrer" href="https://www.github.com/dpalmasan" target="_blank" title="Mi Github">
      <span class="grey fa-brands fa-github fa-lg"></span>
    </a>
  </li>
<li>
    <a rel="me noopener noreferrer" href="https://scholar.google.com/citations?user=Y5PN_1AAAAAJ&hl=en" target="_blank" title="Mi Google Scholar">
      <span class="grey fa-brands fa-google-scholar fa-lg"></span>
    </a>
  </li>
<li>
    <a rel="me noopener noreferrer" href="https://stackoverflow.com/users/4051219/dpalma" target="_blank" title="Mis preguntas en SO LOL!">
      <span class="grey fa-brands fa-stack-overflow fa-lg"></span>
    </a>
  </li>
</ul>
</div>

  </div>

</footer>

</body>

</html>
