---
layout: post
title:  "Motor de b칰squeda de im치genes"
date:   2024-01-06 18:00:00 -0400
categories: python algorithms ir
---

En un post previo describ칤 c칩mo funciona un motor de b칰squeda textual cl치sico y [mostr칠 la implementaci칩n un 칤ndice invertido]({{ site.baseurl }}{% link _posts/2023-01-21-intro-recuperacion-informacion.markdown %}) utilizando noticias de Chile. En mi vida laboral, en una ocasi칩n, utilic칠 dicha t칠cnica + esteroides (por ejemplo `Tf-Idf` y distancia coseno) para un problema en el que ten칤a que hacer calce eficiente entre un repositorio de textos dada una consulta (esto es una reducci칩n del problema, pues era bastante m치s complejo 游땐).

Lo curioso, es que me toc칩 resolver un problema similar pero con im치genes. En este caso hice una integraci칩n de APIs y utilic칠 un modelo pre-entrenado para similitud de im치genes. Sin embargo, el tener una caja misteriosa que me resuelve el problema no me deja satisfecho. Estudiando un poco algunos libros que ten칤a en el estante (y nunca hab칤a mirado 游땐), me inspir칠 e implemente mi propio motor de b칰squeda de im치genes, y este es el tema de este post.

## El Problema de Recuperaci칩n de Im치genes

El problema de recuperaci칩n de im치genes, se puede describir como sigue: _Dado un repositorio de im치genes, y una imagen como consulta, recuperar todas las im치genes relevantes a la consulta_. En este post, consideraremos "relevante" como im치genes que son _similares_ a la imagen de la consulta.

<div align="center">

![cifar-ex](https://gist.githubusercontent.com/dpalmasan/103d61ae06cfd3e7dee7888b391c1792/raw/d8d3eaf7886b931048df96ac88af4fd0626a33be/img-ret-11.png)

_Fig 1: Recuperaci칩n de im치genes relevantes de un repositorio dada una imagen como consulta_

</div>

Algunas aplicaciones:

* B칰squeda de im치genes
* Clasificaci칩n de im치genes
* Calce de contenido (por ejemplo visi칩n artificial)
* Detecci칩n de contenidos abusivos/ilegales en plataformas
* Etc.

### Representaci칩n de Im치genes

Una imagen puede pensarse como un conjunto de capas. Por lo general, se piensa en el conjunto de colores rojo, verde y azul. La idea es, que cualquier color puede representarse como una combinaci칩n de estos tres colores primarios. Por otro lado, una imagen es una grilla de pixeles (matriz), donde cada pixel tiene un color asignado.

### Similitud de Im치genes

Existen varios enfoques para definir una m칠trica de similitud de im치genes. Sin embargo, cada enfoque depende del caso de uso espec칤fico:

1. Histograma de Colores: Por ejemplo, podemos calcular un histograma de cada conjunto de colores (con sus distintas tonalidades), y escoger una cantidad fija de intervalos (_bins_), por ejemplo 32. Si consideramos estos 3 colores, obtendr칤amos un vector de 96 dimensiones, como se muestra:

$$x_i = (x_{i,rojo}^{32}, x_{i,verde}^{32}, x_{i,azul}^{32})$$

Y podr칤amos recuperar las im치genes m치s relevantes mediante una medida de similitud, por ejemplo similitud coseno:

$$sim(x_i, x_j) = cos(x_i, x_j) = \displaystyle \frac{x_i \cdot x_j}{||x_i||||x_j||}$$

2. Vector de pixeles: Asumiendo que las im치genes ser치n de tama침o fijo, considerar un vector de dimension $N$ donde $N$ es la cantidad de p칤xeles de cada imagen. Luego cada componente del vector ser칤a un pixel.

3. Vector de componentes. Aplicar PCA (an치lisis de componentes principales) a la representaci칩n mencionada en el punto previo.

4. Representaci칩n latente (ej. capas internas en una red neuronal)

En los primeros 3 enfoques, no se tiene la informaci칩n contextual, ya que se consideran los pixeles como independientes entre s칤. La ventaja del 칰ltimo enfoque, es que considera que ciertos pixeles pueden tener injerencia (si ya me quiere criticar, le aviso que [es con JOTA](https://dle.rae.es/injerencia) 游땕) sobre pixeles vecinos, lo que lograr칤a hacer comparaciones a nivel de estructura y otras propiedades latentes.

### Implementaci칩n de un Motor de B칰squeda de Im치genes

Para este ejercicio, utilizar칠 el conjunto de datos [CIFAR](https://www.cs.toronto.edu/~kriz/cifar.html), que consiste en im치genes de colores y `32x32` pixeles. Todos los experimentos los ejecut칠 en un notebook en [Colab](https://colab.research.google.com/), y los datos los almacen칠 en mi Google drive.

#### An치lisis Exploratorio

Primero cargamos las dependencias a utilizar. Para instalar FAISS, ejecutar: `!pip install faiss-cpu --no-cache`

```python
from google.colab import drive
import pickle
import faiss
from matplotlib import pyplot as plt
import numpy as np
import torch
from pathlib import Path
from collections import Counter
```

Para montar mi gdrive en la sesi칩n de Colab:

```python
drive.mount('/content/drive')
```

Para cargar el conjunto de datos, sigo las instrucciones en la p치gina de CIFAR:

```python
def unpickle(file):
    with open(file, 'rb') as fo:
        dict = pickle.load(fo, encoding='bytes')
    return dict

IMAGE_FILE_PATH = Path("/content/drive/$DIRECTORIO/cifar-10-batches-py/data_batch_1")
LABELS_FILE_PATH = Path("/content/drive/$DIRECTORIO/cifar-10-batches-py/batches.meta")
label_data = unpickle(LABELS_FILE_PATH)
label_maping = label_data[b"label_names"]
data = unpickle(IMAGE_FILE_PATH)
data.keys()
```

Observamos que los datos est치n almacenados en un diccionario de `python` que tiene las siguientes propiedades:

```python
dict_keys([b'batch_label', b'labels', b'data', b'filenames'])
```

Miremos la distribuci칩n de las etiquetas:

```python
dataset = data[b"data"]
labels = [label_maping[i] for i in data[b"labels"]]
Counter(labels)
```

En este podemos ver que el conjunto de datos est치 casi uniformemente distribuido.

```python
Counter({b'frog': 1030,
         b'truck': 981,
         b'deer': 999,
         b'automobile': 974,
         b'bird': 1032,
         b'horse': 1001,
         b'ship': 1025,
         b'cat': 1016,
         b'dog': 937,
         b'airplane': 1005})
```

Procesamos los datos para tener las imagenes en 3 canales y adem치s las etiquetas de cada imagen:

```python
new_data = []
for img, label in zip(dataset, labels):
    new_data.append((img.reshape(3, 32, 32), label))
```

Echemos una mirada a alguna imagen arbitrariamente seleccionada:

```python
image_example = new_data[3][0].transpose([1, 2, 0])
plt.figure(figsize=(32/50, 32/50))
plt.imshow(image_example)
plt.axis('off')
```

<div align="center">

![cifar-ex](https://gist.githubusercontent.com/dpalmasan/103d61ae06cfd3e7dee7888b391c1792/raw/84520fc82c9db233d6b4d132f01028bbeee065ff/img-ret-1.png)

_Fig 2: Imagen del conjunto de datos CIFAR_

</div>

#### Entrenando un modelo Codificador-Decodificador

Un modelo codificador-decodificador en el caso de las im치genes sigue la siguiente intuici칩n: _Una imagen est치 compuesta de caracter칤sticas latentes (ej. formas, coloraci칩n, etc.). Con estas caracter칤sticas, puedo reconstruir la imagen original_. En este caso, el codificador tiene la tarea de representar la imagen utilizando estas caracter칤sticas latentes, y el decodificador tiene la tarea de, dada dicha representaci칩n, reconstruir la imagen. Esta arquitectura se muestra en la figura 3.

<div align="center">

![cifar-ex](https://gist.githubusercontent.com/dpalmasan/103d61ae06cfd3e7dee7888b391c1792/raw/d8d3eaf7886b931048df96ac88af4fd0626a33be/img-ret-10.png)

_Fig 3: Arquitectura modelo codificador-decodificador_

</div>

En esencia, el modelo debe ser optimizado, de tal forma, que la salida sea idealmente la misma imagen de entrada.

Primero, dividimos el conjunto de datos en un conjunto de entrenamiento y uno de prueba:

```python
train_size = int(0.8 * len(new_data))
test_size = len(new_data) - train_size
train, test = torch.utils.data.random_split(new_data, [train_size, test_size])
```

Verificamos que tenemos los resultados esperados:

```python
print(f"Training set size: {len(train)}")
print(f"Test set size: {len(test)}")
```

Definimos el autoencoder y los par치metros como funci칩n de costo, cantidad de epochs.

**Spoiler**: Utilizo capas convolucionales para considerar el contexto en la imagen de entrada (es decir, pixeles colindantes). Intent칠 hacerlo s칩lo con capas lineales, pero no logr칠 una buena representaci칩n; me queda de tarea pendiente explorar m칰ltiples arquitecturas para ver qu칠 tan buenos resultados se pueden obtener.

```python
class SimpleAutoencoder(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = torch.nn.Sequential(
            torch.nn.Conv2d(3, 32, (3, 3), padding=(1, 1)),
            torch.nn.ReLU(),
            torch.nn.MaxPool2d((2, 2)),
            torch.nn.Conv2d(32, 64, (3, 3), padding=(1, 1)),
            torch.nn.ReLU(),
            torch.nn.MaxPool2d((2, 2)),
        )

        self.decoder = torch.nn.Sequential(
            torch.nn.ConvTranspose2d(64, 32, (2, 2), stride=(2, 2)),
            torch.nn.ReLU(),
            torch.nn.ConvTranspose2d(32, 3, (2, 2), stride=(2, 2)),
            torch.nn.ReLU(),
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded


model = SimpleAutoencoder()
criterion = torch.nn.MSELoss()
num_epochs = 30
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
```

Para cargar los datos en los modelos, `Torch` necesita que se defina un `DataLoader`:

```python
train_loader = torch.utils.data.DataLoader(train,
                                           batch_size=4,
                                           shuffle=True,
                                           num_workers=2)
```

Ahora comienza el entrenamiento del modelo:

```python
# Para almacenar los valores de funci칩n de p칠rdida
train_loss = []
batch_size = len(train_loader)

for epoch in range(num_epochs):
    avg_loss = 0
    for img, label in train_loader:
        # Convertir tipo a float
        img = img.float()
        out = model(img)

        # Comparar la salida del decodificador con la imagen
        # original
        loss = criterion(out, img)

        # Actualizar pesos
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Actualizar promedio de p칠rdida
        avg_loss += loss.item()

    avg_loss /= batch_size
    train_loss.append(avg_loss)
    print(f"Epoch {epoch + 1}|{num_epochs}; Running loss {avg_loss}")
```

Idealmente, la funci칩n de p칠rdida tiene que ir reduci칠ndose a medida que avanza el entrenamiento. Si no es as칤, el modelo no est치 aprendiendo, por lo que habr칤a que revisar la arquitectura del modelo:

```
Epoch 1|30; Running loss 925.2084133262634
Epoch 2|30; Running loss 225.9821763420105
Epoch 3|30; Running loss 194.27336073112488
Epoch 4|30; Running loss 178.33449586868286
Epoch 5|30; Running loss 167.82453124427795
Epoch 6|30; Running loss 160.89200536727904
Epoch 7|30; Running loss 155.2542067222595
Epoch 8|30; Running loss 150.87735285377502
Epoch 9|30; Running loss 147.33312226867676
Epoch 10|30; Running loss 144.14536425971986
Epoch 11|30; Running loss 142.13272936439515
Epoch 12|30; Running loss 139.8893744430542
Epoch 13|30; Running loss 137.89374314308168
Epoch 14|30; Running loss 136.1791773853302
Epoch 15|30; Running loss 134.33099251937867
Epoch 16|30; Running loss 133.34205507850646
Epoch 17|30; Running loss 131.32471635627746
Epoch 18|30; Running loss 130.05829406356813
Epoch 19|30; Running loss 128.98449465942383
Epoch 20|30; Running loss 127.10736661911011
Epoch 21|30; Running loss 126.33275876045226
Epoch 22|30; Running loss 125.31079289817811
Epoch 23|30; Running loss 124.149567653656
Epoch 24|30; Running loss 123.63063527297973
Epoch 25|30; Running loss 122.39257022094726
Epoch 26|30; Running loss 121.8142949180603
Epoch 27|30; Running loss 120.99319108963013
Epoch 28|30; Running loss 120.2384320640564
Epoch 29|30; Running loss 119.61346651077271
Epoch 30|30; Running loss 118.89974891853332
```

Graficamos la curva de aprendizaje:

```python
plt.plot(range(1, len(train_loss) + 1),train_loss)
plt.xlabel("Epochs")
plt.ylabel("P칠rdida en entrenamiento")
plt.title("Curva de aprendizaje del modelo")
plt.show()
```

<div align="center">

![model](https://gist.githubusercontent.com/dpalmasan/103d61ae06cfd3e7dee7888b391c1792/raw/84520fc82c9db233d6b4d132f01028bbeee065ff/img-ret-2.png)

_Fig 4: Curva de aprendizaje del modelo encoder-decoder_

</div>

Ahora a ver qu칠 ocurre con datos de prueba:

```python
test_loader = torch.utils.data.DataLoader(test,
                                          batch_size=1,
                                          shuffle=True,
                                          num_workers=2)
```

Tomamos un s칩lo ejemplo al azar:

```python
with torch.no_grad():
    for img, label in test_loader:
        img_proc = img.float()
        out = model(img_proc)
        break
```

Mostramos la imagen original:

```python
image_example = np.array(img[0]).transpose([1, 2, 0])
plt.figure(figsize=(32/50, 32/50))
plt.imshow(image_example)
plt.axis('off')
```

<div align="center">

![model](https://gist.githubusercontent.com/dpalmasan/103d61ae06cfd3e7dee7888b391c1792/raw/84520fc82c9db233d6b4d132f01028bbeee065ff/img-ret-3.png)

_Fig 5: Imagen original del conjunto de prueba_

</div>

Mostramos la reconstrucci칩n (en un mundo ideal, deber칤a ser igual a la imagen original):

```python
image_example = np.array(out_img).transpose([1, 2, 0])
plt.figure(figsize=(32/50, 32/50))
plt.imshow(image_example)
plt.axis('off')
```

<div align="center">

![model](https://gist.githubusercontent.com/dpalmasan/103d61ae06cfd3e7dee7888b391c1792/raw/84520fc82c9db233d6b4d132f01028bbeee065ff/img-ret-4.png)

_Fig 6: Imagen reconstru칤da del conjunto de entrenamiento_

</div>

#### Extrayendo Representaci칩n Latente (_Embeddings_)

Extraemos representaci칩n latente luego de aplicar el codificador (_embeddings_):

```python
embeddings = torch.Tensor()
images = []
with torch.no_grad():
    for img, label in train_loader:
        img_proc = img.float()
        for im, lbl in zip(img, label):
            images.append((im, lbl))
        enc_output = model.encoder(img_proc).cpu()
        embeddings = torch.cat((embeddings, enc_output), 0)
```

Aplanamos los embeddings para tener vectores:

```python
embeddings_flattened = embeddings.flatten(start_dim=1)
```

En este punto ya tenemos todas las im치genes del conjunto de entrenamiento en una representaci칩n vectorial, basada en la salida del codificador en la red neuronal.

#### Creando un 칈ndice para Recuperar Im치genes Relevantes

Ya tenemos una representaci칩n de la im치gen que el computador puede "procesar" y realizar operaciones matem치ticas. Sin embargo, queda la pregunta, 쮺칩mo recuperamos las im치genes m치s relevantes de la base de datos dado un vector de consulta $x_q$?

<div align="center">

![model](https://gist.githubusercontent.com/dpalmasan/103d61ae06cfd3e7dee7888b391c1792/raw/d8d3eaf7886b931048df96ac88af4fd0626a33be/img-ret-7.png)

_Fig 7: Problema de b칰squeda de im치genes relevantes dada una consulta._

</div>

Un algoritmo ingenuo ser칤a:

1. Inicializar `resultado` como un conjunto vac칤o `[]`
2. Para cada imagen del repositorio:
    - Calcular similitud con la consulta
    - Guardar esta similitud y una referencia a la imagen en `resultado`
3. Ordenar `resultado` de acuerdo a la similitud
4. Retornar `resultado`

Sin embargo, este algoritmo es muy ineficiente. Asumiendo que la dimensionalidad $D$ de los vectores es fija, tendr칤amos que comparar la consulta con todos los vectores de la base de datos $O(n)$. Luego ordenar $O(n \log{n})$. Este enfoque no escala.

Un enfoque m치s "inteligente" podr칤a ser, agrupar las im치genes m치s similares (por ejemplo v칤a _K-Means_), y luego, determinar el centroide m치s cercano a la consulta, y obtener los resultados m치s relevantes s칩lo del subconjunto de im치genes que pertenecen a dicho grupo:

<div align="center">

![model](https://gist.githubusercontent.com/dpalmasan/103d61ae06cfd3e7dee7888b391c1792/raw/d8d3eaf7886b931048df96ac88af4fd0626a33be/img-ret-8.png)

_Fig 8: Agrupando im치genes similares._

</div>

Finalmente, podemos tambi칠n, considerando los centroides, hacer algo un poco m치s "inteligente" y particionar el espacio utilizando diagramas de Voronoi:

<div align="center">

![model](https://gist.githubusercontent.com/dpalmasan/103d61ae06cfd3e7dee7888b391c1792/raw/d8d3eaf7886b931048df96ac88af4fd0626a33be/img-ret-9.png)

_Fig 8: Diagrama de Voronoi del espacio vectorial._

</div>

Las bases de datos basadas en vectores, utilizan principios similares para hacer la b칰squeda de datos m치s eficiente. Por ejemplo un motor de b칰squeda podr칤a utilizar una representaci칩n sem치ntica (_word embeddings_, _sentence embeddings_) para b칰squeda textual. En este caso, se b칰sca el centroide m치s cercano y la b칰squeda se limita a los vectores que se encuentren dentro de dicha partici칩n.

Para implementar la partici칩n utilizando Voronoi, creamos un 칤ndice utilizando la biblioteca `FAISS` (_Facebook Artificial Intelligence Similarity Search_):

```python
nlist = 50
d = embeddings_flattened.shape[1]
quantizer = faiss.IndexFlatL2(d)
index = faiss.IndexIVFFlat(quantizer, d, nlist)
```

Entrenamos el 칤ndice:

```python
index.train(embeddings_flattened)
```

Agregamos im치genes al 칤ndice:

```python
index.add(embeddings_flattened)
```

Obtenemos los embeddings de los datos de prueba:

```python
with torch.no_grad():
    for test_img, test_label in test_loader:
        img = test_img.float()
        enc_output = model.encoder(img).cpu()
        break
```

Escogemos un dato de prueba arbitrariamente y lo consideramos como la consulta a buscar:

```python
query = enc_output.flatten(start_dim=1)
```

Visualizamos la consulta:

```python
image_example = np.array(test_img[0]).transpose([1, 2, 0])
plt.figure(figsize=(32/50, 32/50))
plt.imshow(image_example)
plt.title(test_label[0].decode())
plt.axis('off')
```

<div align="center">

![model](https://gist.githubusercontent.com/dpalmasan/103d61ae06cfd3e7dee7888b391c1792/raw/84520fc82c9db233d6b4d132f01028bbeee065ff/img-ret-5.png)

_Fig 9: Ejemplo de consulta al motor de b칰squeda de im치genes_

</div>

Hacemos la b칰squeda en el 칤ndice definido previamente:

```python
dists, result_indexes = index.search(query, 5)
```

Mostramos los resultados:

```python
rows = 2
columns = 3
padding = 100
fig = plt.figure(figsize=(10, 7))
for fig_num, idx in enumerate(result_indexes[0], 1):
    img = np.array(images[idx][0], dtype=np.uint8).transpose([1, 2, 0])
    fig.add_subplot(rows, columns, fig_num)
    plt.imshow(img)
    plt.title(images[idx][1].decode())
    plt.axis('off')
```

<div align="center">

![model](https://gist.githubusercontent.com/dpalmasan/103d61ae06cfd3e7dee7888b391c1792/raw/84520fc82c9db233d6b4d132f01028bbeee065ff/img-ret-6.png)

_Fig 10: Ejemplo de resultados a la consulta_

</div>

En este caso, para la consulta, los resultados tienen sentido. Por ejemplo, la mayor칤a de los resultados son im치genes de aviones, lo cual es esperado dada la consulta. Sin embargo, tambi칠n el motor recuper칩 im치genes de aves, porque en la representaci칩n latente, estas im치genes son similares a la consulta.

Sin embargo, no hay garant칤as de que todas las consultas entregar치n los resultados esperados. El conjunto de datos tiene sus particularidades, adem치s de haber utilizado im치genes de baja resoluci칩n.

## Conclusiones

* Las im치genes pueden representarse utilizando diferentes enfoques. El mejor enfoque depender치 del caso de uso.
* Las arquitectura codificador-decodificador puede hacer una representaci칩n de las caracter칤sticas latentes de una imagen. La dimensionalidad depender치 de la arquitectura de la red (cuidado que puede chupar mucha RAM 游땐).
* Utilizando representaciones latentes, se pueden crear aplicaciones computacionales interesantes: Clasificador de im치genes, motor de b칰squeda de im치genes, detecci칩n de objetos en im치genes, etc.
* Para encontrar los vectores m치s "cercanos" a un vector consulta, existen varios enfoques y algoritmos. El mejor y m치s eficiente depender치 del caso, del volumen de datos y de la precisi칩n deseada.

## Ejercicios Interesantes

* Hacer un motor de b칰squeda utilizando
    - Representaci칩n basada en histogramas de colores
    - Representaci칩n utilizando pixeles
    - Aplicando PCA/ICA/inserte su algoritmo de componentes principales
    - Utilizando otro tipo de capas (no convolucionales)
* Utilizar la representaci칩n interna para implementar un clasificador de im치genes
